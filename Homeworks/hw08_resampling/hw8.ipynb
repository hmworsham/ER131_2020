{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ER 131] Homework 8: Resampling\n",
    "\n",
    "The past few assignments (HW5, HW6, and HW7) focused on determining model coefficients using different methods (e.g., ordinary least squares regression with scikit-learn, gradient descent...). This assignment takes a step back, and explores resampling techniques. The point of resampling is to figure out how well a model will perform on data it hasn't seen yet. Generally when we resample, we use different subsamples of our observation data to fit a bunch of models of the same form (e.g., linear models), and then look at how the results differ from model to model. This process allows us to tune hyperparameters and calculate values like the variance of our model coefficients or expected value of the mean squared error.\n",
    "\n",
    "We've done a version of resampling in the past (Lab 5), where we randomly subset our data into training and testing sets, but the resampling methods that we're covering now are more systematic than random subsets and are designed to ensure that all of the observations are included when training the models.\n",
    "\n",
    "In this homework, you will work with the cross-validation method was introduced in asynchronous module 8 and in Lab 8. \n",
    "\n",
    "A good reference here is ISLR Ch. 5.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Project](#proj)<br>\n",
    "1. [Overfitting](#of)<br>\n",
    "1. [Cross Validation](#cv)<br> \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Project<a name='proj'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For parts 1.1 -1.3, your group may develop a common response. For 1.4, each team member must provide an individual answer, using a different set of predictor and response variables.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1** A step that can help in defining a project is writing down your response (dependent) and your feature/predictor (independent) variables. \n",
    "\n",
    "1. At this point, what do you think your response variable will be (i.e. what your model is trying to predict)?  \n",
    "2. Based on this variable are you going to be doing regression or classification?\n",
    "2. Does your response variable already exist in a dataset in the form that you need it?  \n",
    "3. How many observations do you expect to have in your sample?  \n",
    "4. What challenges have you encountered in acquiring the data?  \n",
    "\n",
    "If you're choosing between a few options at this point, you can discuss more than one potential response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** Questions to answer:  \n",
    "1. What are the features that you might include in your model? \n",
    "2. Are there any features that you want to include, but you haven't figured out where to get or how to calculate the data? \n",
    "3. Where will the data for the features come from when the model is used for prediction? In other words, in this project, you will use observed $X$ and $y$ data to train, test, and cross-validate a model of the form $\\hat {y_i} = \\hat {f}(X_i)$. Ideally, you would then be able to use your model to predict $y$ in settings (e.g., locations, times) for which you *don't* have access to observations of the response variable. Are feature (i.e., $X$) data available for the settings in which you would want to predict $y$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3** Based on your answer to the questions above, think about what constitutes an \"ideal\" observation of your data. Try to write out your data in the form\n",
    "\n",
    "$$y_i = f(x_{1i}, x_{2i},...) + \\epsilon_i$$\n",
    "\n",
    "where $y_i$ is your response variable and $x_{1i}, x_{2i},...$ are your features. \n",
    "\n",
    "Specific questions to answer:\n",
    "\n",
    "1. What is $y_i$, and what are its units? \n",
    "2. What are $x_{1i}, x_{2i},...$? \n",
    "3. What does $i$ index in your training data?  A point in time, a location in space? (Be as specific as you can!)\n",
    "4. What work do you need to do to make $i$ index the same thing for your response varaible and your features?  As an example of the kind of \"work\" you'd need to do, it's often necessary to do some kind of aggregation (possibly using the `.resample` method in pandas), such as collapsing observations in time or space.  \n",
    "\n",
    "As an example, if we were to answer the questions above for the Novotny et al. paper, we would get:<br>\n",
    "1. $y$ is atmospheric $NO_2$ measurements in $\\mu$g/m$^3$<br>\n",
    "2. $x_1$ is % impervious surface<br>\n",
    "$x_2$ is % tree canopy<br>\n",
    "$x_3$ is population<br>\n",
    "$x_4$ is major road length in km<br>\n",
    "$x_5$ is minor road length in km<br>\n",
    "$x_6$ is total road length in km<br>\n",
    "$x_7$ is elevation in meteres<br>\n",
    "$x_8$ is distance to coast in km<br>\n",
    "$x_9$ is ground-based OMI $NO_2$ measurement in $\\mu$g/m$^3$,<br>\n",
    "and \n",
    "3. In the training data, $i$ indexes locations of the ground-based measurements.\n",
    "4. Many of the features need to be aggregated into buffers around the ground-based measurements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4** Choose two variables of interest from your dataset and visualize the relationship between them using whatever type of plot makes the most sense. Ideally, you would be able to explore the relationship between one of your predictors and your response variable, but if your response variable isn't finalized yet, you can take this as an opportunity to look at the relationship between any two interesting variables.\n",
    "\n",
    "Comment on your plot - is the trend interesting, expected, or surprising at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR CODE OR VISUAL HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR OBSERVATIONS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Overfitting <a name='of'></a>\n",
    "\n",
    "In this class, we have been using mean squared error (MSE), which takes the following form, as a way of quantifying error from our models:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}\\Big(y_i-\\hat{f}(x_i)\\Big)^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is our observed $y$ and $\\hat{f}(x_i)$ is our estimator's best guess of the value $y$.\n",
    "\n",
    "Our goal is to lower the MSE assocated with our model. However, in doing so, we risk overfitting if we minimize MSE on our data without considering how the model might perform on data that it hasn't seen before. As a basic example, we will try to choose a model with two parameters to fit linear data (with some noise added). By now, you should have a good idea of the variables that go into a linear model, and how to fine-tune them to decrease MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create some pseudo-random data for the purposes of this homework. Read the block below before moving on and try to recite to yourself what each variable will represent. The answers for three of the variables are described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block\n",
    "n = 100\n",
    "sigma_2 = 144\n",
    "noise = np.random.normal(scale=np.sqrt(sigma_2), size=n)\n",
    "X = np.random.uniform(-5, 5, n)\n",
    "Y = 50 -2. * X  + 3 * X**2 + 1.5 * X**3 - 0.75 *X**4\n",
    "Y_obs = Y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `n`: the number of datapoints in our space.\n",
    "* `X`: the x-values of our data\n",
    "* `Y`: is the non-biased estimator of our data (i.e. our response variable without any noise added). The model will roughly take its shape, but with a bit of added noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1:** Explain in 1-3 sentences what this line of code does. What do each of the arguments mean? How is the variable defined in this line used later on, and why is it important?\n",
    "\n",
    "```python\n",
    "np.random.normal(scale=np.sqrt(sigma_2), size=n)\n",
    "```\n",
    "\n",
    "*Hint:* This [link](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.normal.html) may help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now set up an experiment where we know the true relationship beteen `Y` and `X`, but we're going to try to predict that relationship using `Y_obs`, which are our observed values of `Y`. Let's start the prediction process with a visualization.\n",
    "\n",
    "\n",
    "**Question 2.2:** Create a scatterplot of the values of `X` and `Y_obs`. You may need to make the dots smaller (or the plot bigger) to avoid too much crowding of datapoints. Be sure to give your plot a title and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3:** When fitting the coefficients to figure out the relationship between `Y_obs` and `X`, we'll start with a general model form. Below is the formula for a second-order model with three parameters.\n",
    "\n",
    "$$ \\Large\n",
    "\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} x_i + \\hat{\\beta_2} x_i^2\n",
    "$$\n",
    "\n",
    "In the block below, describe in one sentence what each variable represents. The first one is done as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:\n",
    "\n",
    "* $\\hat{y_i}$: This is the estimated value of the true $y$ based on our model's parameters.\n",
    "* $\\hat{\\beta_0}$: YOUR ANSWER HERE\n",
    "* $\\hat{\\beta_1}$: YOUR ANSWER HERE\n",
    "* $\\hat{\\beta_2}$: YOUR ANSWER HERE\n",
    "* $x_i$: YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good grasp of what our dataset represents, we are going to try to find a model that best reproduces the relationship between $x$ and $y$. To do this, we need to split our data into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4:** Below, take our data `X` and `Y_obs` and create new data arrays for our new training and test sets. There should be 70% data inside of our training set, and 30% data in our test set. Use the `train_test_split()` function in the `sklearn.model_selection` library (also used in Lab 8) with a random state of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "assert [np.size(X_train), np.size(y_train)] == [70, 70]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5** Create a function, `scatter()`, that plots the training and testing data on the same scatter plot. As input, it should take in `X_train`, `X_test`, `y_train`, `y_test`, and a title. This will be useful later on, when we'll be outputting the scatter plot but overlaying different model fit lines on it - it will help to avoid copy-pasting code.\n",
    "\n",
    "Use different markers or marker colors to denote training and testing data, and add a legend, axis labels, and a title (the title should change based on the input to the function).\n",
    "\n",
    "After you've defined your function, call it in the second cell to produce a scatter plot of training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "def scatter(X_train, X_test, y_train, y_test, title):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "scatter(X_train, X_test, y_train, y_test, \"X vs Y_obs, divided into training and testing sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to fit our first model! What if we weren't sure what the general model form looked like? Normally, you'd start with the most simple model and improve from there. So, that is exactly what we will do - we will create a ***feature matrix,*** $\\Phi$. $\\Phi$ is a matrix with $d$ columns, where each column contains the values of $X$ raised to a power from 1 to $d$. The general form of $\\Phi$ looks like this:\n",
    "\n",
    "$$ \\large\n",
    "\\Phi_d(x) = \\left[x, x^2, \\ldots, x^d \\right]\n",
    "$$\n",
    "\n",
    "For a first order model, $d$ would be equal to 1 and $\\Phi$ would look like this:\n",
    "\n",
    "$$ \\large\n",
    "\\Phi_1(x) = \\left[x \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Similarly, for a second order model, $d$ would be equal to 2 and $\\Phi$ would be:\n",
    "\n",
    "$$ \\large\n",
    "\\Phi_2(x) = \\left[x, x^2 \\right]\n",
    "$$\n",
    "\n",
    "Depending on how many features (degrees) we are adding to our model, we account for it using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_phi(d, X):\n",
    "    #return lambda X: np.array([X ** i for i in range(1, k+1)]).T\n",
    "    return np.array([X ** i for i in range(1, d+1)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the function `poly_phi()` to our data, let's create a simple array `X_sample`. Run the cell below and make sure you understand how `poly_phi()` is working. Try changing the value for `d` to see how the output matrix changes for different degree values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = np.array([1,2,3])\n",
    "\n",
    "poly_phi(1,X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6**: Define a function `lm_phi()` that performs a linear regression using a $\\Phi_d$ matrix. \n",
    "* As input, it should take X_train, X_test, y_train, y_test, and d (the degree of matrix $\\Phi_d$). \n",
    "* It should produce a scatter plot of the training and testing data (using the function `scatter()`) with a curve overlayed that shows the result of the linear regression. The plot's title should indicate the highest degree of the polynomial (e.g., \"Order 1 Linear Model on Train and Test Data\" for a first-order model). \n",
    "* It also should return the mean squared error of the test data and of the training data.\n",
    "\n",
    "*Hint*: Here's the documentation for the `LinearRegression` object again: [docs](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "*Hint*: When it comes to plotting a the curve of a model in matplotlib, it sometimes make sense to create a range of `X` values, `X_plot`, then generate predictions for each of those. If you generate predictions for your `X_test` values and plot those, matplotlib will basically \"connect the dots\", but since the `X_test` values aren't sorted, you'll end up with a plot that has lines crossing each other. If, instead, you define a range of values `X_plot`, then generate predictions for each of those, you'll get a curve - matplotlib is still connecting the dots, but now that it's been given a sequential set of values, it will do so in a way that actually reflects the shape of the function. To get you started, we've defined a variable `X_plot` below that you can use to generate predictions.<br>\n",
    "\n",
    "When you calculate the test MSE, however, you should be using `X_test` (and getting predictions based on those values) rather than `X_plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def lm_phi(d, X_train, X_test, y_train, y_test):\n",
    "    phi_train = poly_phi(...) # get the phi matrix of the training data\n",
    "    \n",
    "    # fit a model using training data\n",
    "    lm = ...\n",
    "    lm.fit(...)\n",
    "    \n",
    "    X_plot = np.linspace(np.min(X_test), np.max(X_test), 200)\n",
    "    \n",
    "    y_pred = ...\n",
    "    \n",
    "    title = ... # plot title\n",
    "    scatter(...) # create scatter plot\n",
    "    plt.plot(...) # plot best fit curve\n",
    "    \n",
    "    MSE_test = ... # calculate MSE of test data\n",
    "    MSE_train = ... # calculate MSE of training data\n",
    "    \n",
    "    return MSE_test, MSE_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7**: Call `lm_phi()` with `d = 1` to fit a model using the $\\Phi_1$ matrix. Print the testing and training MSEs.\n",
    "\n",
    "Your figure will look something like this:\n",
    "<img src='hw8_img.png' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model with `d = 1` is a good start, but we're **underfitting** the model - we're not quite capturing the underlying trend of the data when we fit just the one feature. How do we fix this?\n",
    "\n",
    "Let's try a different approach: instead of fitting just one feature, we'll fit many. Let's try creating a 32nd-order polynomial estimator. \n",
    "\n",
    "**Question 2.8**: Call `lm_phi()` with `d = 32` to fit a model using the $\\Phi_{}$ matrix. Print the testing and training MSEs. After you've called `lm_phi()`, try using `plt.ylim()` to \"zoom in and out\" of your plot and get a better look at how the model varies with `X`, and output the plot with the y axis limits that best show how the model is responding to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.9** How did the values for training and test MSE change when you changed your feature matrix from having one column (`d = 1`) to 32 columns (`d = 32`)? What do the testing and training MSEs indicate about the relative bias and variance of the two models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.10** Try running `lm_phi()` with different values for `d`. Choose the model that provides (visually and based on MSE values) the best fit of training and testing data. \n",
    "\n",
    "*Note*: you don't have to systematically loop through all values for `d` and choose the optimal one - we'll do that in the next section. You can just try different values for `d` and settle for one - you will probably find that there's a small range of values for `d` that give roughly similar model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Section 3: Cross Validation<a name='cv'></a>\n",
    "\n",
    "You have now estimated the best fit for the data we created. Let's think about the steps we took to get to a successful answer: \n",
    "\n",
    "1. First, we started with a simple first-order polynomial to have a baseline training error.\n",
    "1. We noticed from our graphing that it had high bias, so we continued to add more features...\n",
    "1. ... until we reached a point in which both the training data and the test data would both be well-fit with the number of features we have chosen.\n",
    "\n",
    "In Section 2, we were trying to visually figure out which model best fit both the training and test sets. However, we won't always be able to rely on just visualization for all cases. This is where k-fold cross validation (CV) comes in!\n",
    "\n",
    "\n",
    "**k-fold cross validation** is a systematic way to make sure that we train the model at least once on every point in the dataset.\n",
    "\n",
    "<img src='kfoldcv.png' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "The figure above, from ISLR pg. 181, shows an example of 5-fold cross validation. A k-fold cross validation algorithm randomly splits the data into k groups, ensuring that they're non-overlapping (i.e. each of the k groups has a validation set, and each validation set contains a distinct set of observations). In the figure above, each beige section represents about a fifth of the data set, and the model is fit 5 times with the remaining 4/5 of the data. We'll end up with 5 values for MSE, and we can calculate metrics like the average MSE or the variance of MSE to assess how the model responds to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** Let's practice splitting training data into k-folds for validation purposes. Split the array `X` from Section 2 into 4 folds, shuffling before we add the batches, with a random state of 13. For each fold, print the indices of the Train and Validation sets onto the console.\n",
    "\n",
    "Here's the [documentation for KFold()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "kf = KFold(...)\n",
    "\n",
    "fold = 1\n",
    "for train_i, val_i in kf.split(...):\n",
    "    # print training indices\n",
    "    # print validation indices\n",
    "    fold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that each test set is distinct from the others, meaning that none of the test sets share common items even though they are shuffled pretty well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2:** Let's do a little review first before you start implementing the k-fold CV. Complete the function below to calculate the mean squared error between an array of predicted and true y-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def mse(y_pred, y_true):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall back to Section 2 when you tried different numbers of features to train models. We can use k-fold CV to find out which model has the lowest mean squared error when predicting y-values. Let's define an array, `d_array`, of possible `d` values to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "d_array = np.arange(1,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3:** Complete the function `mse_k_fold_lr` below. We are seeking to create a new k-fold CV pipeline that will give us the information we need to evaluate which features to include in our model based on the average cross-validated MSE of several models. \n",
    "* The inputs to the function include the number of folds (given by input `k`), the array of possible d values to evaluate (`d_array`, defined above), the `poly_phi` function defined in Section 2, the `mse` function defined in Question 3.2, the $X$ and $y$ arrays, and a random state given by the integer input `rand_state`. \n",
    "* Make sure to shuffle the indices when creating your folds.\n",
    "* Your function should return a dictionary in which each key represents the degree of the highest polynomial in the model (i.e., the d-value associated with a model), and the value is the average value MSE associated with that model across all k folds. The dictionary should be saved in a variable called `average_mses`.<br>\n",
    "* Your function should also return the d (degree) value that minimizes MSE across all models. \n",
    "\n",
    "This is a somewhat long function, but the code skeleton is written for you - focus on understanding what each step is trying to achieve (from the comments and from your understanding of k-fold cross-validation) and filling in code where you see ellipses (...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def mse_k_fold_lr(k, d_array, poly_phi, mse, X, y, rand_state):\n",
    "    '''This function will print the d (degree) value that gives the smallest MSE.\n",
    "    Returns the dictionary of average MSEs (from 3-fold cross validation) across the specified d values\n",
    "    '''\n",
    "    \n",
    "    # 1. K-Fold where k = k\n",
    "    kf = KFold(...)\n",
    "    \n",
    "    # 2. Save the MSEs of each split.\n",
    "    mses = np.full((k,len(d_array)),np.nan)\n",
    "    # above, we're initializing an array where:\n",
    "    # every row of mses corresponds to one of the folds\n",
    "    # every column of mses corresponds to one of the possible d values in d_array\n",
    "    # for example, mses[0,0] corresponds to the mean squared error for the first fold, using d = 1 (the first element of d_array)\n",
    "    \n",
    "    fold = 0 # initialize fold value\n",
    "    for train_i, val_i in kf.split(X): # this loop iterates through the K folds of our data\n",
    "        # 2.1 Separate X and Y_obs array into testing and validation sets\n",
    "        X_fold_train = ...\n",
    "        y_fold_train = ...\n",
    "        X_fold_val = ...\n",
    "        y_fold_val = ...\n",
    "        \n",
    "        # 2.2 Each d value will receive its own LinearRegression() model.\n",
    "        for i in range(len(d_array)): # this loop iterates through d values to produce a new model for each d value\n",
    "            # 2.2.1 Initialize the model\n",
    "            model = ...\n",
    "            \n",
    "            # 2.2.2 Fit the training data using feature matrix with number of degrees equal to d[i] and the y_fold_train\n",
    "            model.fit(...)\n",
    "            \n",
    "            # 2.2.3 Predict y-values for the test set of features transformed into a feature matrix\n",
    "            y_pred = model.predict(...)\n",
    "            \n",
    "            # Save each mse between y_pred and y_fold_val at their respective fold and d value\n",
    "            mses[...] = ...\n",
    "        \n",
    "        fold += 1 # augment the fold count\n",
    "    \n",
    "    # 3. Now, find the average of the MSEs for each d value. Your result should be a vector of the same length as d_array,\n",
    "    # in which each element is the average MSE across all k folds associated with that d-value.\n",
    "    average_mses = {}\n",
    "    for i in range(len(d_array)):\n",
    "        average_mses[d_array[i]] = ...\n",
    "    \n",
    "    # 4. Find the index of the minimum average MSE\n",
    "    min_mse_index = min(average_mses, key=average_mses.get)\n",
    "    \n",
    "    print(\"Minimum MSE Parameters:\", min_mse_index, '\\n',\n",
    "          \"MSE of {} Parameters:\".format(min_mse_index), average_mses[min_mse_index])\n",
    "    return average_mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "average_mses = mse_k_fold_lr(4, d_array, poly_phi, mse, X_train, y_train, 13)\n",
    "average_mses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** The function call above for 4-fold cross validation on the training data with a random state of 13, should have returned a minimum MSE parameters of 4, with an MSE of about 144. In the cell below, try calling `mse_k_fold_lr()` but:\n",
    "\n",
    "* changing the `rand_state` input but keeping `k` constant\n",
    "* changing `k` to a larger value, and trying different values of `rand_state`\n",
    "\n",
    "Then, answer the following questions:\n",
    "\n",
    "* What are you changing in terms of the implementation of k-fold CV when you change `k`? How about when you change `rand_state`?\n",
    "* Do the results (minimum MSE parameters and associated MSE) change when you vary `k` or `rand_state`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5:** Use the `matplotlib` or `seaborn` libraries to visualize the average *ROOT* mean squared error (RMSE) from cross-validating and to confirm the degree of the polynomial that minimizes MSE from Question 3.3. Use `k` = 4, `rand_state` = 13. Your plot should have a marker where the RMSE is minimized, and should have axis labels and a title.\n",
    "\n",
    "*Hint*: you may want to plot the log of the RMSE to clearly display the full range of RMSE values\n",
    "\n",
    "*Hint*: you can extract values from a dictionary using `.values()`, and can convert dictionary values to a list using `list()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Congrats, you're done with Homework 8!\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file **as both an .html and .ipynb file through bCourses**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
