{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ER 131] Homework 6: Land Use Regression and Model Selection\n",
    "\n",
    "\n",
    "In this homework, we will continue to build linear regression models. Instead of the single-variable regression models of Homework 5, we will build multi-variable regression models. Specifically, we will work with the Novotny et al. (2011) data to build and analyze the performance of land use regression (LUR) models that predict nitrogen dioxide concentration near the Earth's surface.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Project](#section1)<br>\n",
    "1. [Multiple linear regression using land-use regression data](#section2)<br>\n",
    "1. [Model selection](#section3)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Section 1: Project <a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, your group should work collaboratively to refine your research question and identify candidate data sources. You may develop answers to Questions 1.1 and 1.2 as a group, but if you do so, please identify each member's unique contribution (for example: \"Jessica summarized reports or datasets #1-2, Duncan summarized reports or datasets #3-4...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1** Give some context for your prediction problem. Have you come across any work that answers questions that are similar or related to the ones that you are asking? What results have they found? What are you hoping to do differently from other researchers who have asked similar questions?<br>\n",
    "\n",
    "We're definitely not expecting you to review a lot of academic papers and projects for this question, but you should take a look around to see if there are any papers or reports that ask similar questions or use similar data - beyond giving the reader context for your project in your final report, looking at other people's work can give you ideas for how to approach your own project. \n",
    "\n",
    "One way you could approach this question is for each team member to identify and summarize 1-2 relevant citations. Summaries should focus less on the specifics of other papers and more on how findings in the literature inform and motivate your research question. Ultimately, this information can inform the introduction and motivation sections of your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** Identify, open, and summarize your group's candidate datasets. Try to find $1+N_{s}$ relevant data sources, where $N_{s}$ is the number of students in your group. For **each** dataset:\n",
    "- Insert a link to and a brief description of the data. \n",
    "- Open the dataset and incorporate the dataset into a Pandas dataframe if feasible.\n",
    "- Grab some descriptive statistics about the dataset using `pd.describe()`. Paste the output below (you can also load it and run `pd.describe()` below, but if it's a very large dataset you might hit the memory limit, in which case you should load and inspect it in a separate notebook and then paste the output below). What do you notice when you run `pd.describe()`? Is there anything surprising or expected about the output?\n",
    "\n",
    "No need to do a full-scale EDA at this point (we'll do that next week!); this week's focus is on making sure you can open and summarize your candidate datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Section 2: Multiple linear regression using land-use regression data <a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remainder of this homework, we will dig into the data used by [Novotny et al. (2011)](https://bcourses.berkeley.edu/files/78396490/download?download_frd=1). We'll use these data to explore multiple linear regression, land use regression, and the important questions one has to ask when conducting these types of analyses and interpreting results.\n",
    "\n",
    "We'll be using two different libraries: `scikit-learn`, and `StatsModels`. `scikit-learn` is preferred in the machine-learning community, and is easier to use for methods concerning prediction (e.g., cross validation). `StatsModels` is preferred in the statistics and econometrics communities, shares syntax closer to R, and generally provides more statistical information.\n",
    "\n",
    "**Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "pd.set_option('display.max_columns', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1** Let's start by reading in the .csv file \"BechleLUR_2006_finalmodel.csv\", found in the data folder, as a Pandas dataframe named `df`. Print its first few rows.<br>\n",
    "\n",
    "These are the data used in the Novotny et al. (2011) paper. The dataframe contains the response and predictor variables, as well as the model results (ie., the predicted variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** If your goal is to use multiple linear regression to predict NO2 levels, which column represents the response variable (i.e., the \"truth\" that you are trying to predict)? Which columns are the independent variables (i.e., features)? State in words what each response and independent variable represents, along with its unit of measurement. The Novotny et al paper is a good reference here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3** Let's filter our dataframe to make it easier to do multiple linear regression. We will not be using all columns in the dataframe `df` as independent or response variables - specifically, we will ignore Monitor_ID, Latitude, Longitude, State and Predicted_NO2_ppb. Create a new dataframe, `df_clean`, that does not include these variables, and print the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4** There is one qualitative variable in our dataframe. Which one is it? What are its possible values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH WORK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the qualitative categories of the variable you identified in Question 2.4 into a set of binary variables using the Pandas one-hot encoder. \n",
    "\n",
    "**Question 2.5** What is the minimum number of binary variables you will need to create in order to represent that categorical data in the qualitative variable you identified in Question 2.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6** Replace the ellipsis in the cell below to create a dataframe in which the columns are the binary variables (corresponding to the categories of our qualitative variable), the rows correspond to observations, and the elements are either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vars = pd.get_dummies(...) # YOUR CODE HERE\n",
    "binary_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert binary_vars.shape == (369,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7** Replace the qualitative variable in your `df_clean` dataframe with the set of binary variables you produced in Question 2.6. In other words, `drop` the column containing the quantitative variable and `concat` the columns you produced in 2.6. Do not change the name of `df_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.8** Now, let's use `scikit-learn` to fit our linear model. In the cell below, fit a linear model using the response and independent variables in `df_clean`. The process will be very similar to the process for fitting a linear model (call it `sk_model`) using a single response variable (see Lab and Homework 5). Save the output of `.fit()` to `sk_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your model's intercepts and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Intercept\n",
    "print(\"Intercept:\", ...)\n",
    "# Coefficients\n",
    "print(\"Coefficients:\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how scikit-learn is very simple to use, but is not always informative - in this case, we aren't told which column each these coefficients corresponds to. In order to get that information, we are going to run linear regression using the `statsmodels` library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.9** In the cell below, fit $X$ and $y$ to a linear model using `statsmodels`. The skeleton code below will get you started, but you should also check out the [documentation for linear modeling in statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html).<br> \n",
    "\n",
    "Don't forget to add a column of 1's to $X$ so that `statsmodels` can fit an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X2 = sm.add_constant(...)\n",
    "sm_model = sm.OLS(...)\n",
    "results = sm_model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.10** A good check of whether or not you've set up your `statsmodels` regression properly is if the coefficient and intercept values match up with those output by `scikit-learn`. Compare your outputs from 2.8 and 2.9. Were your intercepts and all of the coefficients the same across the two methods? If not, why do you think this might be the case? How might you change your inputs for fitting either `scikit-learn` or the `statsmodels` to make the outputs match?\n",
    "\n",
    "*Hint:* If you're stuck, go back and listen closely to video W6.2.2 of the asynchronous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH WORK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.11** Examine the bounds of the 95% confidence interval provided in the output from Question 2.9 (i.e., the \"[0.025\" and \"0.975]\" columns). For which of the independent variables would you be most skeptical that a relationship exists between that variable and the response variables? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Section 3: Model selection <a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've produced a multiple regression model, we can think about model selection. Model selection is the process of choosing a subset of independent variables to include in a regression model. To do this, we need a search strategy: how are we going to systematically include or exclude different variables in our model? \n",
    "\n",
    "One way to assess a model is using the Aikake Information Criterion ($\\text{AIC}$). The $\\text{AIC}$ assesses the ***quality*** of a model. Depending on the data that we use in our model - in this case, the data associated with the independent variables we add - AIC tells us how our model performs. Sometimes adding more data (i.e., independent variables) improves quality, and sometimes it doesn't. \n",
    "\n",
    "We define $\\text{AIC}$ as the following:\n",
    "\n",
    "$\\text{AIC} = 2 \\times (\\text{number of features}) - 2 \\times \\ln(\\text{maximum value of likelihood function})$\n",
    "\n",
    "A likelihood function tells us what the maximum likelihood is that the coefficients that we have chosen will predict the true $y$ value. We don't go into it in much depth, but we will provide the code to calculate it.\n",
    "\n",
    "The smaller $\\text{AIC}$ is, the better the model performance. One way to think about it: if $\\text{AIC}$ is small, the likelihood function is high - i.e., there's a high likelihood that the coefficients predict the observed $y$ value. And if we have one model that uses less features, and another that uses more features, but they have the same likelihood function, then the model that uses less features has a smaller AIC value. AIC thus defines models that have a relatively high probability of predicting the observed values, while using relatively few features, as high quality models.\n",
    "\n",
    "$\\text{AIC}$ is important because we can use it as a benchmark for model selection. **Our goal is to find a model that has the highest *quality*--i.e., the lowest AIC.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1** Load the file \"allmodelbuildingdata.csv\" into a Pandas dataframe. This dataframe that contains all the features that were in `df` as well as additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df_all = ...\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2** Fill in the code below to compute the AIC using the log likelihood function. `statsmodels` returns log likelihood from the fitted model using the right syntax. In the function definition below, `fit_model` represents the output of a call of `statsmodels` `.fit()` method (eg. the `results` variable that we defined above to get the multiple regression). `k` represents the number of features in the model.\n",
    "\n",
    "*Note*: Yes, `statsmodels` also returns AIC directly, but we'd like you to do at least *a little* work to compute AIC here! Check the [attributes section of the linear regression documentation](https://www.statsmodels.org/stable/regression.html) to figure out how to grab the likelihood value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAIC(fit_model,k):\n",
    "    llf = ... # get likelihood\n",
    "    AIC = ... # calculate AIC\n",
    "    return AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3** Use `computeAIC` to compute the AIC of the `results` model from Question 2.9 of the homework. Check that your result matches the AIC given in the `statsmodels` summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated earlier, the lower the AIC the better. Let's choose our own features and see if we can create a model that has a comparable AIC; we can start off choosing a few features and see what we get.\n",
    "\n",
    "\n",
    "**Question 3.4** Choose the features `WRF+DOMINO`, `Distance_to_coast_km`, `Elevation_truncated_km`, `Impervious_100`, and two more features of your choice. Then, fit this model and calculate the AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try computing a model with fewer features.\n",
    "\n",
    "**Question 3.5** From the previous model, keep only `WRF+DOMINO`, `Distance_to_coast_km`, and `Impervious_100` and calculate the AIC. Did the quality of your model improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6** Make a plot that shows model quality and the likelihood function as a function of the number of independent variables. Your plot will have two subplots: the y-axis of the first will be AIC, and the second will be the log likelhood function. The x-axis of both subplots is $k$, ranging from k = 1 to the total number of features in `df_all`. You can approach this however you want, but you do have to explain your approach - specifically, how did you choose which features to add for each $k$ value? Do you notice any trends in the AIC and likelihood values? Can you explain that trend, based on what you know about how AIC is calculated?<br>\n",
    "\n",
    "*Note*: we're not asking you to calculate AIC for every combination of independent variables, just for different numbers of independent variables (features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR APPROACH HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # YOUR PLOT HERE\n",
    "# fig, (ax0,ax1) = plt.subplots(nrows=2, sharex=True, figsize = (12,8))\n",
    "\n",
    "# ax0.plot(...,..., color = 'navy')\n",
    "# ax0.set_xlabel(...)\n",
    "# ax0.set_ylabel(...)\n",
    "# ax0.set_title(...)\n",
    "\n",
    "# ax1.plot(...,..., color = 'gold')\n",
    "# ax1.set_xlabel(...)\n",
    "# ax1.set_ylabel(...)\n",
    "# ax1.set_title(...)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.7** Approximately how many features does the highest quality model have, based on AIC? How many features maximize the likelihood that the model predicts the true response variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Submission\n",
    "\n",
    "Congrats, you've finished Homework 6! \n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file through bCourses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Joshua Asuncion, revised by Jessica Katz\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
