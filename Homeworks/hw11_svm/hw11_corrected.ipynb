{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ER 131] Homework 11: Support Vector Machines\n",
    "----\n",
    "\n",
    "This homework will use support vector machines to classify CalEnviroScreen data. We will take gradual steps in this homework, starting from recalling key information from lectures and textbook, to creating our own classifiers. Throughout the homework, we'll learn about the intuition behind the Perceptrons and Maximal Margin Classifiers (MMC), then move on to learning about the intuition behind support vector machines (SVMs) and applying them to CalEnviroScreen data. The textbook reference here is ISLR 9.1-9.3.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[CalEnviroScreen Data](#data)<br>\n",
    "1. [Perceptrons and MMC](#perceptron)<br>\n",
    "1. [SVM Intuition](#svm)<br>\n",
    "1. [Using SVM to Classify CalEnviroScreen Data](#classify) <br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import Samples Generator\n",
    "from sklearn.datasets.samples_generator import make_circles\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Color Scheme for SVM!\n",
    "colormap = np.array(['blue', 'gold']) # Go bears!\n",
    "\n",
    "# Allows us to plot SVC decision functions\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\n",
    "    \n",
    "    Variables: \n",
    "        model: classifier\n",
    "    \n",
    "    Usage:\n",
    "    >>> from sklearn.svm import SVC\n",
    "    >>> clf = SVC(kernel='linear')\n",
    "    >>> clf.fit(X, y)\n",
    "    >>> plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "    >>> plot_svc_decision_function(clf) # Draw the decision boundary\n",
    "    >>> plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none');\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### CalEnviroScreen Dataset <a id='data'></a>\n",
    "Carrying on from the previous homework, we will be using the CalEnviroScreen dataset. CalEnviroScreen 3.0 is a comprehensive documentation of the environmental and the demographic conditions of each census tract in California. In this homework, we are interested in predicting environmental conditions using information related to demographics. \n",
    "\n",
    "Please note that the Excel file can be downloaded from [here](https://oehha.ca.gov/calenviroscreen/report/calenviroscreen-30). However, for this homework, the Excel file has already been placed in the same directory as the homework. \n",
    "\n",
    "Before we get to working with the data, however, we're going to use simulated data to develop some concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1: Perceptrons and Maximal Margin Classification  <a id='perceptron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's a Perceptron?\n",
    "\n",
    "You'll remember from the asynchronous lectures that SVM are a way to classify observations into one of two possible classes. SVMs are a pretty flexible method that can allow for non-linear splits between observations. Given a set of hyperparameters, the SVM delivers a unique solution. \n",
    "\n",
    "For the purposes of this homework, a perceptron is any hyperplane sitting between linearly separable data. In this way perceptrons are a little more generic than SVMs (any plane will do) and a little less flexible (the data need to be linearly separable).  \n",
    "\n",
    "We use the following mathematical notation when we talk about perceptrons. We'll define our training dataset as $D$, and the number of points in $D$ as $n$, with the following notation:\n",
    "\n",
    "$D = \\{(x_i, y_i)\\}_{i = 1}^{n}$\n",
    "\n",
    "$x_i$ and $y_i$ have to meet certain criteria: $y_i$ can only be equal to -1 or 1, where -1 represents one class and 1 represents another class. This is expressed mathematically as:\n",
    "\n",
    "$y_i \\in \\{-1 , +1\\}$\n",
    "\n",
    "We also specify that $x_i$ has to be a real number (this is not a condition that we really have to worry about in our applications of machine learning) in this way:\n",
    "\n",
    "$x_i \\in R^p$ \n",
    "\n",
    "$p$ is the number of features we have - i.e. $X$ is a $n \\times p$ matrix.\n",
    "\n",
    "A perceptron is a $p - 1$-dimensional hyperplane that perfectly separates $+1$ and $-1$. A hyperplane is defined as a \"flat subspace of a $p$-dimensional space.\" \n",
    "\n",
    "Let's think about what this means intuitively, by considering different values of $p$. If $p = 1$, that means we're trying to divide a single set of predictors into two categories. In this case, a $(p - 1)$-dimensonal hyperplane is a 0-dimensional hyperplane - which in our case is just a single point! You can think of your $x$ values as falling along a number line, and your division being a single point on that line. If $p = 2$, we're dividing two sets of predictors into two categories, so we want a 1-dimensional hyperplane - this is a line. In this case, you can think of plotting your first predictor $x_1$ on an x-axis, and $x_2$ on a y-axis; a line can be drawn to separate observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1** If we have 3 predictors, what is the shape and dimension of a hyperplane that divides our observations into two classes? How would you plot this hyperplane?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next question, we are going to use `make_blobs` and `make_circles` extensively. These are sample generators made by `scikit-learn` package, which--as their names imply--will allow us to randomly generate blobs and circles.\n",
    "\n",
    "The following cell is an example of how we might call `make_blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating blobs of data with 2 centers\n",
    "X, y = make_blobs(n_samples=100, centers=2, cluster_std=0.50, center_box=(-4, 4), random_state = 2020)\n",
    "\n",
    "# Plotting the blobs of data\n",
    "plt.title(\"Data of Two Randomly Generated Clusters\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2:** There are many hyperplanes that can separate these two clusters of data. Give 3 possible examples in the code below based on visually inspecting the plot above. We've started defining three tuples for you (`first`, `second`, and `third`), where the first item in each tuple is the slope and second item should be the y-intercept. In other words, `first/second/third = (slope, y-intercept)`.\n",
    "\n",
    "(*Side note:* We talked about tuples at the start of the semester -- but here's a reminder: They're kind of like lists, in that they contain a sequence of values through which you can iterate, but unlike lists the values they contain can't be modified easily after they're initialized. Tuples are contained in round brackets (`()`) while lists are contained in square brackets (`[]`))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example hyperplanes here\n",
    "first = (..., ...)\n",
    "second = (..., ...)\n",
    "third = (..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to double-check that your answers are right and reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "xfit = np.linspace(X[:,0].min(), X[:,0].max())\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
    "\n",
    "for m, c in [first, second, third]:\n",
    "    plt.plot(xfit, m * xfit + c, '-k')\n",
    "\n",
    "plt.title(\"Two Randomly Generated Clusters and Hyperplanes Separating Them\")\n",
    "plt.xlim(xfit.min()-0.5, xfit.max()+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 1.3: </b> There are multiple answers (we definitely know there are at least 3!) to Question 1.2. Explain why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4:** The issue explored above - i.e., that a dataset can allow for multiple hyperplanes that divide it - lead us to use maximal marginal classification (MMC) or hard-margin SVM in practice. In a few sentences, explain what differentiates MMC from the perceptrons that we've explored above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to examine how to code Perceptrons, especially since this process is very similar to that used to code SVMs (which we will see later in the homework). First, we import the necessary library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our artificial dataset which has been hard-coded below. Later, we will use `scikit-learn`'s `samples_generator` to create our datasets. But, for now, let's try to develop a better understanding of the kind of data that we use to classify information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "[2, 1, 2, 5, 8, 2, 3, 6, 1, 2, 5, 5, 5, 5, 6],\n",
    "[2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 4, 7, 5, 7, 3]\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5:** Qualiatatively describe what `x` and `y` represent in the above block of code. What will the coordinate $(1, 3)$ be classified as?\n",
    "\n",
    "*Hint*: `x` and `y` above function similarly to the `X` and `y` that were assigned as the output of `make_blobs()` a few questions ago!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have developed an understanding of what our data is, let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Data colored by class\")\n",
    "plt.scatter(x[0], x[1], c=colormap[y], s=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6:** Based on the graph, is this dataset linearly separable? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to try building code the finds perceptrons, using a modified dataset defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "[2, 1, 2, 5, 7, 2, 3, 6, 1, 2, 5, 4, 6, 5],\n",
    "[2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 7]\n",
    "])\n",
    "\n",
    "label = np.array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n",
    "plt.title(\"Data colored by class\")\n",
    "plt.scatter(data[0], data[1], c=colormap[label], s=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.7:** In the following block, set up our classifier, using `Perceptron`. Look at the following [link](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) for more information on setting up a classifier with Perceptron. We want the model to have the following parameters:\n",
    "1. `max_iter` = 100\n",
    "2. `verbose` = 0\n",
    "3. `eta0` = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # YOUR CODE HERE\n",
    "classifier = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.8** In the following block, fit the Perceptron classifier you set up in Question 1.7. Again, please refer to the following [link](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) for more information on using the classifier. \n",
    "\n",
    "Before we run `.fit()`, take a look at the above documentation for `Perceptron()`. Which variables will we use as the `X` and `y` inputs? Are their dimensions correct? If not, make any adjustments needed and then run `classifier.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # YOUR CODE/ANSWER HERE\n",
    "...\n",
    "classifier.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the following cell to see what kind of decision boundary we have come up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data\n",
    "plt.scatter(data[0], data[1], c=colormap[label], s=40)\n",
    "\n",
    "# Calc the hyperplane (decision boundary)\n",
    "xmin, xmax = plt.xlim()\n",
    "\n",
    "w = classifier.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(xmin, xmax)\n",
    "yy = a * xx - (classifier.intercept_[0]) / w[1]\n",
    "\n",
    "# Plot the line\n",
    "plt.plot(xx, yy, 'k-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.9** In this question, repeat the steps in questions 1.7-1.8, but this time pass a value other than the default to the arguments `penalty` and `n_iter_no_change` (read the docstring or documentation above to figure out possible arguments for `penalty`. Pass a value greater than 5 to `n_iter_no_change`). How does the decision boundary change? What is changing about our implementation of the algorithm when we modify these parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Perceptron Algorithm implemented here is satisfied when it finds *any* hyperplane that separates the data.  You can probably see that other hyperplanes would leave more \"room\" between the data at the hyperplane.  That's where maximal margin algorithms (which underlie SVM) come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2: SVM Intuition <a id='svm'></a>\n",
    "\n",
    "Before we start classifying the CalEnviroScreen Dataset, let's review the intuition behind using Support Vector Machines. \n",
    "\n",
    "<img src=\"svd.png\" width=\"400\">\n",
    "\n",
    "This is an example of an artificially created data-set to be classified, where red $+$ data points and green $\\large{\\circ}$ data points represent two different classes. In the following questions, assume the following: \n",
    "1. Training data comes from error-prone sensors, so we're not that confident in the location of any one point,\n",
    "2. We are training our SVM using a __quadratic__ kernel. The hyperparameter $C$ will determine the location of the separating hyperplane.\n",
    "\n",
    "Answer the following questions with a one line justification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1** Given the potential decision boundaries below, which one has a large $C$ (i.e. $C \\to \\infty$) and which one has a small $C$ (i.e. $C \\to 0$)?\n",
    "\n",
    "<img src=\"svd2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** In this particular dataset, why might it be more advantageous to use a large value of $C$ than a small $C$ one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3** Imagine you've received one additional data point in the green circle class. Name one coordinate this data point could have that will **not change** the decision boundary for small values of C. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4** Name one coordinate the new data point could have that **will** change the decision boundary learned for small values of $C$. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin the next set of questions, run the following cell to import the module we need to run SVM using `scikit-learn`. We will also call the different libraries that we will be using in this question, namely `samples_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we are going to use `make_blobs` again. Run the cell below to generate another set of random blobs.\n",
    "\n",
    "Keep this code in mind as you will later be asked to call `make_circles`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating blobs of data with 2 centers\n",
    "X, y = make_blobs(n_samples=50, centers=2, cluster_std=0.60, center_box=(-4, 4), random_state = 11)\n",
    "\n",
    "# Plotting the blobs of data\n",
    "plt.title(\"Data of Two Randomly Generated Clusters\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use support vector machines to classify our data. \n",
    "\n",
    "**Question 2.5** Make the Support Vector Machine Classifier in the following cell with a linear kernel.\n",
    "\n",
    "*Hint:* Use `SVC` to create your model and refer to this [link](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) for more information. **Important note:** Pay close attention to the definition of the parameter `C` as applied in scikit-learn! It's not quite the same as the hyperparameter $C$. \n",
    "\n",
    "Then, fit your support vector machine on the data. Remember, the data for blobs is `X` and labels are `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # YOUR CODE HERE\n",
    "clf = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at what we have made by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing SVM and Data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colormap[y], s=50)\n",
    "plot_svc_decision_function(clf)\n",
    "\n",
    "plt.title(\"SVM on the Blobs of Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only really dealt with linearly separable data. What happens if the data is not linearly separable? Let's examine these cases in the following questions. \n",
    "\n",
    "**Question 2.6** Just as we have created our dataset for blobs of data, make a new dataset for circles below. Plot that data, using different colors for the different classes of data.\n",
    "\n",
    "*Hint: Refer to the code above and use the function `make_circles`[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # YOUR CODE HERE\n",
    "X, y = make_circles(100, factor=.1, noise=.2)\n",
    "\n",
    "# # plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7**: Train this new dataset using a linear kernel and plot the points along with the decision boundary, using the function `plot_svc_decision_function()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel doesn't really do a good job of distinguishing these two classes. Let's try a radial basis kernel, called `\"rbf\"` in scikit-learn. \n",
    "\n",
    "**Question 2.8** Repeat the steps in question 2.7, but with a radial basis kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 3: Using SVM to Classify CalEnviroScreen Data <a id='classify'></a>\n",
    "\n",
    "<br>\n",
    "Now that we've explored how SVM works and how to implement it, let's begin applying our knowledge on the CalEnviroScreen dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "env = pd.read_excel('ces3results.xlsx')\n",
    "demog = pd.read_excel('ces3results.xlsx', sheet_name = 3, header = 1)\n",
    "demog = demog.rename(columns = {'Unnamed: 0': 'Census Tract',\n",
    "                          'Unnamed: 1': 'CES 3.0 Score',\n",
    "                          'Unnamed: 2': 'CES 3.0 Percentile',\n",
    "                          'Unnamed: 3': 'CES 3.0 Percentile Range',\n",
    "                          'Unnamed: 4': 'Total Population',\n",
    "                          'Unnamed: 5': 'California County'\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's have a look at what each of these dataframes contains (these should look familiar from HW10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, we'll select only the columns we want. We'll be predicting the Percentile Range based on Unemployment and PM2.5, and we'll only be looking at records with a percentile > 95% or < 5%. Run the cell below to grab just those columns and rows and remove any NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "env_model = env[['Unemployment', 'PM2.5','CES 3.0 \\nPercentile Range']]\n",
    "env_model = env_model.loc[env_model['CES 3.0 \\nPercentile Range'].isin(['95-100% (highest scores)', '1-5% (lowest scores)'])]\n",
    "env_model.dropna(inplace = True)\n",
    "env_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** Now, given the dataframe `env_model`, create a dataframe `X` that contains the predictors and `y` that contains the response. Then, split `X` and `y` into train and test sets, using an 80/20 train/test split and a `random_state` of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2:** Let's start classifying information now. Below, like *Question 2.7*, make a SVM classifier with a linear kernel (choose $C$ of your choice! If it takes too long, it is probably too high) and train the data. \n",
    "\n",
    "*Hint: If you get the error,* \n",
    "\n",
    ">`DataConversionWarning`: A column-vector ?? was passed when a `1d` array was expected. Please change the shape of ?? to (n_samples, ), for example using `ravel()`\n",
    "\n",
    "*use ??.values.ravel() to override this issue*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3:** Use the classifier to predict the outcome of our `X_test` and save the output to `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have completed training a data-set. Run the box below to see if you are getting a reasonable percentage of correct matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** Interpret the values in the confusion matrix. What does each of the four values mean? The [documentation for `confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) is a good reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5** Based on the classification report, are there more false positives for the \"lowest score\" class or are there more false negatives? Which values in the classification report give you this information?\n",
    "\n",
    "For reference, see the documentation for [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html); scrolling down to the \"Returns\" section and following relevant links there will also be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see what our classification looks like! Note that this code assumes that `y` is a Pandas series and `X` is a Pandas dataframe; if you set up your `X` and `y` differently you may have to make slight modifications where those variables are called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "classes = pd.unique(y)\n",
    "y_label = np.array([np.where(classes == i)[0][0] for i in y.values])\n",
    "\n",
    "labels = [None]*len(y_label)\n",
    "for i in classes:\n",
    "    labels[np.where(y == i)[0][0]] = i\n",
    "\n",
    "for i in range(len(X)):\n",
    "    plt.scatter(X.iloc[i, 0], X.iloc[i, 1], c=colormap[y_label[i]], s=20, label = labels[i])\n",
    "\n",
    "plot_svc_decision_function(svclassifier)\n",
    "\n",
    "plt.xlabel(\"unemployment\")\n",
    "plt.ylabel(\"PM2.5\")\n",
    "plt.title(\"SVM on Cal Enviroscreen Data\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission\n",
    "\n",
    "Congrats, you finished the final homework!\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file through bCourses.\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "Carnegie Mellon University's Machine Learning Course (10 - 701) - Images on Question 2 - http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml\n",
    "\n",
    "Jayanta Basak, *A Least Square Kernel Machine with Box Constraints* - Inspiration and Formula for Question 4.4 - http://www.jprr.org/index.php/jprr/article/viewfile/181/57\n",
    "\n",
    "Jake VanderPlas - Function for Drawing SVC Decision Boundaries - *Python Data Science Handbook*\n",
    "\n",
    "---\n",
    "Notebook developed by: Beom Jin Lee (Brian)\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
