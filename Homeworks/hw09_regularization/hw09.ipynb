{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ER 131] Homework 9: Model Selection and Regularization\n",
    "\n",
    "This assignment explores methods that answer the question of how to choose which features to include in a model. In HW8 (resampling), we used cross-validation to evaluate how well different models generalize to new data, and one of the big takeaways was that using too many features can cause us to over-fit our model on the training data, and lead to a model that performs poorly on test data. In this HW, we'll look at two methods that attempt to reduce model variance by prioritizing certain model features: ridge regression, and lasso regression.\n",
    "\n",
    "In this lab, we will be writing lots of functions. While these exercises take time and patience, you may find that the resulting functions are helpful in your final project (especially if you are working on a regression problem). \n",
    "\n",
    "A good reference for this HW is ISLR Ch. 6.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [The data](#data)<br>\n",
    "1. [Comparing linear, ridge, and lasso regression](#models)<br>\n",
    "1. [Standardizing your variables](#standardization)<br>\n",
    "1. [Choosing lambda](#lambda)<br>\n",
    "1. [Comparing optimal models](#compare)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Data<a name='data'></a>\n",
    "\n",
    "We're going to be working (for the final time!) with the Novotny et al. LUR data. You will recognize it from HW6 and Lab 9. As a refresher:\n",
    "\n",
    "* The data is an accumulation of GIS land-use characteristics from land-monitoring by the EPA and in situ NO2 measurements from satellite sensors.\n",
    "* The goal of this land-use regression (LUR) is to estimate outdoor air pollution geospatially across the contiguous United States.\n",
    "* The reason for the high number of data points is that the data keeps track of readings from monitors at a high resolution, up to ~30 meters.\n",
    "\n",
    "First, let's upload the data to dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T18:50:14.728090Z",
     "start_time": "2018-08-13T18:50:08.632835Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this block\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "df = pd.read_csv('data/BechleLUR_2006_allmodelbuildingdata.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we're going to be creating models for two sets of data: one for the full dataset, and one for just California data. We'll be looking at how different approaches (lasso, and ridge) perform with datasets that have more or fewer observations.\n",
    "\n",
    "**Question 1.1** To get started, create a dataframe `df_ca` that contains just California observations from dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_ca.shape == (94,135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** As with the lab and previous HW, we want to predict Observed_NO2_ppb and we want to use all the columns except for Monitor_ID, State, Latitude, Longitude, Observed_NO2_ppb, and Predicted_NO2_ppb as features.\n",
    "\n",
    "Write a function `get_X_y()` that takes as input a dataframe, and returns four dataframes: X_train, X_test, y_train, and y_test. Remember to start by selecting the X values (all the columns except for the ones listed above) and the one y value (observed NO2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T18:50:30.895466Z",
     "start_time": "2018-08-13T18:50:30.862262Z"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_X_y(df, cols_to_drop, y_col, test_prop, rand_seed):\n",
    "    \"\"\"\n",
    "    This function returns four dataframes containing the testing and training X and y values used in land-use regression.\n",
    "    Input: df, a Pandas dataframe with all of the fields in the land-use regression dataset; \n",
    "        cols_to_drop, a list of the names (strings) of the columns to drop from df in order to obtain the feature variables.\n",
    "        y_col, a column name (as a string) of df that represents the response variable\n",
    "        test_prop, a float between 0 and 1 indicating the fraction of the data to include in the test split\n",
    "        rand_seed, an integer, used to define the random state\n",
    "    Returns: X_train, X_test, y_train, y_test, four dataframes containing the training and testing subsets of the \n",
    "    feature matrix X and response matrix y\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    X = ...\n",
    "    y = ...\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3** Call your `get_X_y` function on `df` and `df_ca`, and save the resulting dataframes to `X_all_train, X_all_test, y_all_train, y_all_test` and `X_ca_train, X_ca_test, y_ca_train, y_ca_test` respectively.\n",
    "\n",
    "Write your function to hold 25% of the data as test data, with a `random_state` of 2020 as input.  This is important!  The assert functions won't work if you don't use that random state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_all_train, X_all_test, y_all_train, y_all_test = ... # get X and y train and test dataframes for full dataset\n",
    "                                                          \n",
    "X_ca_train, X_ca_test, y_ca_train, y_ca_test = ... # get X and y train and test dataframes for CA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions of output dataframes\n",
    "assert X_ca_train.shape[0] == y_ca_train.shape[0]\n",
    "assert X_all_test.shape[0] == y_all_test.shape[0] == 93\n",
    "assert X_ca_test.shape[1] == X_all_test.shape[1] == 129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Comparing linear, ridge, and lasso regression<a name='models'></a>\n",
    "\n",
    "Our feature matrix has a very high number of features in it (129 to be exact). It would be a big hassle to determine the importance of each independent variable (that would take ages!). So how should we go about indentifying which features to include in our model?\n",
    "\n",
    "As a recap from Lab 9 and from lecture, for OLS regression, a model with two features would take a form that looks something like this:\n",
    "\n",
    "$$\\Large \\hat{y} = \\hat{\\theta_1} x_{1i} + \\hat{\\theta_2} x_{2i}$$\n",
    "\n",
    "However, when we have a large number of features (the number of features $p$ approaches the number of observations), OLS can result in high variance, leading to overfit. How would we fix this problem?\n",
    "\n",
    "Enter **regularization.** We will begin with **ridge regression ($L^2$ regularization)**. This method finds the values of $\\theta$ that minimize mean squared error plus a **regularization** or **penalty** term. Visually...\n",
    "\n",
    "$$ \\large \\hat{\\theta} = \\arg \\min_\\theta \\frac{1}{n} \\sum_{i=1}^n \\textbf{Loss}\\left(y_i, \\hat{y_i}\\right) + \\lambda {R_{L^2}}(\\theta) $$\n",
    "\n",
    "where $\\large R_{L^2}(\\theta) = \\sum_{k=1}^p (\\theta_k)^2$ and $p$ is the number of features.\n",
    "\n",
    "In the case of Lasso regression, we would uses the penalty term $\\large R_{L^1}(\\theta) = \\sum_{k=1}^p \\Big|\\theta_k\\Big|$.\n",
    "\n",
    "In both Ridge and Lasso regression, $\\lambda$ is a hyperparameter that we can tune to balance the bias-variance tradeoff. The higher the value of $\\large \\lambda$, the more a model is penalized for including additional features. Essentially, we are decreasing variance and increasing bias with a large $\\lambda$ value, and vice versa for a small $\\lambda$ value.\n",
    "\n",
    "For the purposes of `sklearn`, the lambda value will be passed in as an argument `alpha`.\n",
    "\n",
    "What value of $\\lambda$ would be lead to the right-sized penalty term? We need to check a few answers to find a good option for this term. What is a technique that we know that can will check for the \"best fit\" across different terms? You guessed it: **cross-validation**! We'll be doing cross validation later this notebook. For now, let's see how our models perform on a random train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1** In this homework we will compare the results of three models: simple linear regression, Ridge regression, and Lasso regression. As we know from Lab 9, `scikit-learn` has very generalizable syntax when it comes to using different models: regardless of which type of regression you are performing, you instantiate a model and save that instance of the model to a variable (eg. `lm = LinearRegression()`), then use `.fit()` to fit that model to the training data and `.predict()` to output predictions for test data.\n",
    "\n",
    "In this question, we're going to complete a function `fit_model()` that automates the process of initializing a model, fitting that model, and getting predictions. The function should return two sets of values: the model coefficients and the test mean squared error of the model.\n",
    "\n",
    "We've gotten you started with some skeleton code. It may be helpful to test out each part of the function separately, and then put it all together. It also may be helpful to look back at Lab 9 to see how we initialize and fit different models.\n",
    "\n",
    "*Hint:* When getting the coefficients of a model using `.coef_`, sometimes scikit-learn returns a 1D array and sometimes it returns a nested array (i.e. with double brackets). To make sure that the dimensions of the coefficients that you output using this function are always the same, consider using the [numpy .flatten() method](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html).\n",
    "\n",
    "*Another hint:* We're fitting one of three potential models here: LinearRegression(), Ridge(), and Lasso(). While Ridge() and Lasso() take in an alpha argument, LinearRegression() does not! That means your code needs to check which type of model you'll be fitting before initializing that model and (if Lasso or Ridge) passing an alpha argument to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_model(Model, X_train, X_test, y_train, y_test, alpha = 1):\n",
    "    \"\"\"\n",
    "    This function fits a model of type Model to the data in the training set of X and y, and finds the MSE on the test set\n",
    "    Inputs: \n",
    "        Model (sklearn model): the type of sklearn model with which to fit the data - LinearRegression, Ridge, or Lasso\n",
    "        X_train: the set of features used to train the model\n",
    "        y_train: the set of response variable observations used to train the model\n",
    "        X_test: the set of features used to test the model\n",
    "        y_test: the set of response variable observations used to test the model\n",
    "        alpha: the penalty parameter, to be used with Ridge and Lasso models only\n",
    "    \"\"\"    \n",
    "    \n",
    "    # initialize model\n",
    "        \n",
    "    # fit model\n",
    "    \n",
    "    # get mean squared error of test data\n",
    "    mse = ...\n",
    "    \n",
    "    # get coefficients of model\n",
    "    coef = ...\n",
    "    \n",
    "    return mse, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to confirm that fit_model() works as expected\n",
    "\n",
    "# check the MSE of using Ridge regression on the full data - the value should be ~ 28 if your random_state was 2020 in get_X_y\n",
    "assert 27 < fit_model(Ridge, X_all_train, X_all_test, y_all_train, y_all_test)[0] < 28\n",
    "\n",
    "# make sure that the number of coefficients is equal to the number of features\n",
    "assert len(fit_model(LinearRegression, X_all_train, X_all_test, y_all_train, y_all_test)[1]) == 129\n",
    "\n",
    "# check that Lasso is reducing the coefficients as expected - i.e. by setting some coefficients to zero\n",
    "assert np.count_nonzero(fit_model(Lasso, X_all_train, X_all_test, y_all_train, y_all_test)[1]) < len(fit_model(Lasso, X_all_train, X_all_test, y_all_train, y_all_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** For each set of data (i.e., the training and testing sets of `X_all, y_all` and `X_ca, y_ca` obtained in Section 1), call `fit_model()` 3 times: once with `Model = LinearRegression`, once with `Model = Ridge` and once with `Model = Lasso`. For Ridge and Lasso, you can leave the `alpha` value as its default value (1). Save both the MSE and the coefficient output everytime you run the models. You'll end up with 6 MSE values and 6 sets of coefficients (3 for the full dataset and 3 for California; 1 per model type in each case). It's up to you how you store the values (you can use lists or arrays or separate variable names, as long as you're able to access the MSE values and coefficients later in the lab). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T18:51:21.727317Z",
     "start_time": "2018-08-13T18:51:21.529644Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3** Create a plot with two subplots below. In the first subplot, plot the coefficients for Ridge regression, Lasso regression, and simple linear regression for the full dataset. In the second, plot the same, but for the California data. Skeleton code is provided to construct a bar plot, but you can change this to use whatever type of plot makes the most sense to you. Make sure to have axis labels, titles, and legends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ellipses with your code\n",
    "\n",
    "# Set up the bar plot arrangement\n",
    "ind = np.arange(coef_all.shape[0])\n",
    "width = 0.25\n",
    "pos = np.array([ind - width, ind, ind + width])\n",
    "modelNames = [\"Linear regression\", \"Ridge\", \"Lasso\"]\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "# Plot the coefficients for Linear, Ridge, and Lasso models fit using all data\n",
    "plt.subplot(211)\n",
    "for i in np.arange(coef_all.shape[1]):\n",
    "    plt.bar(x = ..., height = ..., width = ..., label = ...)\n",
    "plt.legend()\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "plt.title(...)\n",
    "\n",
    "# Plot the coefficients for Linear, Ridge, and Lasso models fit using CA data\n",
    "plt.subplot(212)\n",
    "for i in np.arange(coef_all.shape[1]):\n",
    "    plt.bar(x = ..., height = ..., width = ..., label = ...)\n",
    "plt.legend()\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "plt.title(...)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4** How do the coefficients from lasso, ridge, and linear regression compare to each other for the full dataset? How about for the California data? Based on your knowledge of lasso, ridge, and linear regression, can you explain why the coefficients compare in this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5** Compare the 3 test MSES from the full dataset and from the California dataset. How do the MSEs compare within model types (e.g., how does the MSE for linear regression fit on the California model compare with the linear regression fit using all the data)? Can you explain how the two sets of values compare, based on what you know about bias and variance? \n",
    "\n",
    "Particularly, note how the MSEs change for the California data when Lasso is used vs. for all data when Lasso is used. Which set of data exhibits a larger improvement when Lasso is used? Why do you think that is?\n",
    "\n",
    "You don't have to use a visualization for this question, although you are welcome to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Standardizing your variables<a name='standardization'></a>\n",
    "\n",
    "As you may recall from lecture, it's a good idea to standardize your data when you are using regularization methods, especially when the domains of the independent variables span dramatically different ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1** Use the `.describe()` method on the original `df` dataframe and examine the results. Provide at least two examples of features whose values span dramatically different ranges (i.e., at least one order of magnitude). In your own words (and referring to the asynchronous content as needed, describe why we should standardize our data when we are using regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for us, `sklearn` has built a built-in method, `StandardScaler` that makes it easy to standardize our variables before we fit our regression models. You can read more about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html ). In essence, `StandardScaler` takes in our array of features. For each feature (i.e., column), the method takes the difference between each element and the mean value of the feature and divides the result by the standard devation of the features, according to the following equation:\n",
    "\n",
    "$$\n",
    "X_{stnd} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation, respectively, of the feature $X$. \n",
    "\n",
    "[Side note: Strictly speaking, we should only use `StandardScaler` if our features are normally distributed. If that's not the case, `sklearn` offers other scaling methods. For further reading, [see this article](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02 ).]\n",
    "\n",
    "The following code block shows an example of how to use `StandardScaler` to standardize the first three features from the Novotny et. al data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_test = df.iloc[:,6:9] # We'll standardize the WRF+DOMINO, Distance_to_coast_km, and Elevation_truncated_km features\n",
    "\n",
    "scaler = StandardScaler() # Initialize the StandardScaler\n",
    "scaler.fit(X_test) # Fit the standard scaler for each feature\n",
    "print(scaler.mean_) # Print the mean value of each feature\n",
    "\n",
    "X_stnd = scaler.transform(X_test) # Standardize each feature\n",
    "X_stnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2** Using the code above as a template, update your `get_X_y` function to standardize all of the features before splitting the data into testing and training sets. Then, call your new `get_X_y_stnd` function on `df` and `df_ca`, and save the resulting dataframes to `X_all_train, X_all_test, y_all_train, y_all_test` and `X_ca_train, X_ca_test, y_ca_train, y_ca_test` respectively. Again, use a test_size of 0.25 and random seed of 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_stnd(df, cols_to_drop, y_col, test_prop, rand_seed):\n",
    "    \"\"\"\n",
    "    This function returns four dataframes containing the testing and training X and y values used in land-use regression.\n",
    "    It standardizes the X variables prior to splitting the data.\n",
    "    Input: df, a Pandas dataframe with all of the fields in the land-use regression dataset; \n",
    "        cols_to_drop, a list of the names (strings) of the columns to drop from df in order to obtain the feature variables.\n",
    "        y_col, a column name (as a string) of df that represents the response variable\n",
    "        test_prop, a float between 0 and 1 indicating the fraction of the data to include in the test split\n",
    "        rand_seed, an integer, used to define the random state\n",
    "    Returns: X_train, X_test, y_train, y_test, four dataframes containing the training and testing subsets of the \n",
    "    feature matrix X and response matrix y\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_train, X_all_test, y_all_train, y_all_test = ... # get X and y train and test dataframes for full dataset\n",
    "X_ca_train, X_ca_test, y_ca_train, y_ca_test =  # get X and y train and test dataframes for CA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert -1.4 < X_all_train[:,0].all() < 4.7\n",
    "assert 0 < y_ca_test['Observed_NO2_ppb'].all() < 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3** Replicate your process for Questions 2.2 and 2.3, but this time use your newly standardized data. Your result should be a plot with two subplots (one for the full dataset and the other for CA only), similar to the figure you produced in 2.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE to get the coefficients and MSE for each model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE to plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** How do your coefficients change for linear regression, Ridge, and Lasso models when you standardize your features relative to when you do not? Can you explain these changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scratch work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Choosing lambda<a name='lambda'></a>\n",
    "\n",
    "To further tune our model for maximum predictive power, we'll now use cross-validation to tune our hyperparameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1:** In this question, you'll complete a function `model_cv_mse()`. This function should perform k-fold cross validation on the training data, which is passed to the function through `X` and `y`, using the model (`Ridge()` or `Lasso()`; no LinearRegression this time) specified by the input `Model` for each value of alpha in the list `alphas`. It should then calculate the MSE associated with each alpha in `alphas`, and return a list of mean squared errors.\n",
    "\n",
    "This time, use a `random_state` of 9 for `KFold`, and set `shuffle = True`.\n",
    "\n",
    "This function will end up looking a lot like `mse_k_fold_lr()` from HW8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cv_mse(Model, X, y, alphas, k = 3):\n",
    "    \"\"\"\n",
    "    This function calculates the MSE resulting from k-fold CV using Lasso or Ridge regression performed on a training subset of \n",
    "    X and y for different values of alpha.\n",
    "    Inputs: \n",
    "        Model (sklearn model): the type of sklearn model with which to fit the data - LinearRegression, Ridge, or Lasso\n",
    "        X: the set of features used to fit the model\n",
    "        y: the set of response variable observations\n",
    "        alphas: a list of penalty parameters\n",
    "        k: number of folds in k-fold cross-validation\n",
    "    Returns:\n",
    "        mses: a list containing the mean squared cross-validation error corresponding to each value of alpha\n",
    "    \"\"\"\n",
    "    mses = ... # initialize array of MSEs to contain MSE for each fold and each value of alpha\n",
    "        \n",
    "    kf = KFold(...) # get kfold split\n",
    "    \n",
    "    fold = 0\n",
    "    for train_i, val_i in kf.split(X):\n",
    "        # get training and validation values\n",
    "        X_f_train = ...\n",
    "        X_f_val = ...\n",
    "        y_f_train = ...\n",
    "        y_f_val = ...\n",
    "        \n",
    "        for i in range(len(alphas)): # loop through alpha values\n",
    "            model = ... # initialize model\n",
    "\n",
    "            model.fit(...) # fit model\n",
    "            \n",
    "            y_pred = ... # get predictions\n",
    "            \n",
    "            # save MSE for this fold and alpha value\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    average_mses = ... # get average MSE for each alpha value across folds\n",
    "    \n",
    "    return average_mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "alphas_test = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "# check dimensions of output MSE\n",
    "assert len(model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test)) == len(alphas_test)\n",
    "\n",
    "# check values of output MSE at first alpha - should be about 10.4\n",
    "assert 10 < model_cv_mse(Lasso, X_all_train, y_all_train, alphas_test)[0] < 11\n",
    "\n",
    "# check reproducibility - the function should give the same MSE for the same alphas every time it runs\n",
    "assert np.array_equal(model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test),\n",
    "                       model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2** Now that we can calculate the cross-validated MSE for different values of alpha, we can visualize the relationship between alpha and cross-validated MSE. Below, complete the skeleton code to create two subplots for the full dataset: the first subplot should show alpha vs. cross-validated MSE for Ridge regression for the training data, and the second should show alpha vs cross-validated MSE for Lasso regression for the training data. Use 5-fold CV.\n",
    "\n",
    "It's up to you what range of alphas you want to show, but your plot should aim to roughly include the range around which the minimum MSE value appears. As a starting point, look at the output MSE values using the `test_alphas` values in the `assert` code block above, and try to decide which alphas to \"zoom in\" on from there - you may also decide to extend the range of alphas. It may take some trial and error.\n",
    "\n",
    "*Hint*: You will end up with two very different ranges of alpha for lasso and ridge. \n",
    "\n",
    "Make sure to add axis labels and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_ridge = ...\n",
    "alphas_lasso = ...\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(...)\n",
    "plt.title(...)\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(...)\n",
    "plt.title(...)\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Question 4.3** Comment on the plots above. How does the MSE change with different alphas for ridge vs lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR OBSERVATIONS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4:** In what cases would you expect ridge regression to perform better than lasso, and vice versa? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR OBSERVATION HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Comparing optimal models<a name='compare'></a>\n",
    "\n",
    "In this final section, we'll compare the performance of three models: your Lasso model with optimal alpha, your Ridge model with optimal alpha, and Novotny's model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.1:** Let's start by getting the optimal alpha and corresponding coefficients of the Ridge model. In this section, we'll use a new scikit-learn function, [`RidgeCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html). `RidgeCV()` takes as an input a list of alpha values and a cross-validation splitter object (which is just the output of the function `KFold(...)`).\n",
    "\n",
    "To call `RidgeCV()`, you would want to first create a cross-validation object using `Kfold()`:\n",
    "\n",
    "`kf = KFold(n_splits = ..., shuffle = ..., random_state = ...)`\n",
    "\n",
    "You can then pass `kf` to `RidgeCV()`, along with a list of alpha values:\n",
    "\n",
    "`ridgecv = RidgeCV(cv = kf, alphas = ...)`\n",
    "\n",
    "and you can treat it essentially like any other `scikit-learn` model by calling `.fit()`:\n",
    "\n",
    "`ridgecv.fit(X_train, y_train)`\n",
    "\n",
    "Running the code above will, in one line, perform cross-validation, choose an ideal alpha from your list, and select the corresponding coefficients. You can then access the optimal alpha using:\n",
    "\n",
    "`ridgecv.alpha_`\n",
    "\n",
    "and the corresponding coefficients using:\n",
    "\n",
    "`ridgecv.coef_`\n",
    "\n",
    "You can also get predictions on your test data using `.predict()`:\n",
    "\n",
    "`ridgecv.predict(X_test)`\n",
    "\n",
    "In the cell below, we've written some skeleton code to get you started. You'll want to call `RidgeCV()` using 5-fold cross validation with a random state of 9, train the model on the training data for the **full** dataset (i.e. `X_all_train, y_all_train`), get predictions for the test data (i.e. `X_all_test`), and find the mean-squared-error on your cross-validated ridge model on the test data. Save the mean squared error to variable `ridgecv_mse`.\n",
    "\n",
    "You may also want to print the optimal alpha value to make sure it's consistent with your plot in question 4.2.\n",
    "\n",
    "You can use the same range of alphas that you did for plotting, or you can choose to use a smaller, larger, or more granular range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "kf = KFold(...) # get KFold cross-validation selector object\n",
    "ridgecv = RidgeCV(cv = ..., alphas=...) # pass CV object and list of alphas to RidgeCV()\n",
    "ridgecv.fit(...) # fit RidgeCV model on training data\n",
    "\n",
    "alpha_opt = ... # get optimal alpha value\n",
    "print(\"optimal alpha:\", alpha_opt)\n",
    "\n",
    "y_pred_ridgecv = ... # get test predictions using RidgeCV model\n",
    "\n",
    "ridgecv_mse = ... # get MSE of RidgeCV model\n",
    "print(\"Test MSE with cross-validated Ridge:\", ridgecv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.2:** Now, let's get the optimal alpha and corresponding coefficients of the Lasso model. We'll use the function [`LassoCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). As in question 5.1, `LassoCV()` takes as an input a list of alpha values and a cross-validation splitter object (which is just the output of the function `KFold(...)`). You would call `LassoCV()` and use it to fit a model, get predictions, and access the alpha and coefficient values in the same way you would `RidgeCV()`.\n",
    "\n",
    "In this section, you'll want to call `LassoCV()` using 5-fold cross validation with a random state of 9, train the model on the training data for the **full** dataset (i.e. `X_all_train, y_all_train`), get predictions for the test data, and find the mean-squared-error on your cross-validated Lasso model on the test data. Save the mean squared error to variable `lassocv_mse`.\n",
    "\n",
    "You may also want to print the optimal alpha value to make sure it's consistent with your plot in question 4.2. Note that it's not unusual if LassoCV() returns an optimal alpha that's slightly different than the alpha that shows up at the minimum point in your plot in question 4.2, due to how LassoCV() solves for coefficients. However, the alpha value should be within a reasonable range of the minimum alpha you get in question 4.2.\n",
    "\n",
    "Again, you can use the same range of alphas that you did for plotting, or you can choose to use a smaller, larger, or more granular range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "kf = KFold(...) # get KFold cross-validation selector object\n",
    "lassocv = LassoCV(cv = ..., alphas=...) # pass CV object and list of alphas to LassoCV()\n",
    "lassocv.fit(...) # fit RidgeCV model on training data\n",
    "\n",
    "alpha_opt = ... # get optimal alpha value\n",
    "print(\"optimal alpha:\", alpha_opt)\n",
    "\n",
    "y_pred_lassocv = ... # get test predictions using RidgeCV model\n",
    "\n",
    "lassocv_mse = ... # get MSE of LassoCV model\n",
    "print(\"Test MSE with cross-validated Lasso:\", lassocv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside: at this point, you might be asking yourself why we wrote the function in Question 4.1 when we can use `RidgeCV()` and `LassoCV()`. While `RidgeCV()` and `LassoCV()` are very useful for getting optimal alphas and corresponding coefficients and mean squared errors, it's harder to extract the cross-validated mean squared error for a *list* of alphas from those functions (particularly from `LassoCV()`). We wanted you to have both tools (a self-written function that calculates MSE across alphas, and scikit-learn provided functions that return the optimal alpha and coefficients) at your disposal.\n",
    "\n",
    "Moving on!\n",
    "\n",
    "**Question 5.3** Now, let's see how Novotny's model performed. The list `X_nov` below are the predictors that the authors put in their final model data set.  Use that list to fit a multiple linear regression model whose predictors are just the predictors listed in `X_nov`.  Fit the linear regresion model to the training dataset we've been using throughout the lab (`X_all_train, y_all_train`) but select only the columns in `X_nov` from that training set. Then calculate the mean squared error between the model's prediction on the test data set (i.e. `X_test`, but containing only the columns in `X_nov`) and the actual data for the test set.  Save the MSE value to `novotny_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this block; the numbers in X_nov correspond to the column names of the X_all datasets post standardization.\n",
    "\n",
    "\n",
    "# X_nov = ['WRF+DOMINO',\n",
    "#        'Impervious_800', 'Elevation_truncated_km', 'Major_800',\n",
    "#        'Resident_100', 'Distance_to_coast_km']\n",
    "\n",
    "X_nov = [0, 10, 2, 54, 69, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_nov_train = ...\n",
    "X_nov_test = ...\n",
    "\n",
    "n...\n",
    "\n",
    "print(\"Test MSE with Novotny model:\", novotny_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.4** Which model performs best?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.5** In this question, we compared the MSE of 3 models. Name and describe one other metric we could use to compare models. How do you think the relative model performance of ridge, lasso, and Novotny's model would differ using that metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Submission\n",
    "\n",
    "Congrats, you're done with homework 9!\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file **as both an .html and .ipynb file through bCourses**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Alex Nakagawa, editted by Jessica Katz\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
