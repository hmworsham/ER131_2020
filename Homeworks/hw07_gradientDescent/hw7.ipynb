{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99241281e589ab3c08a8d3b926a35ae1",
     "grade": false,
     "grade_id": "intro",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# [ER 131] Homework 6: Gradient Descent\n",
    "\n",
    "This homework focuses on gradient descent. \n",
    "\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "* [Project](#project)<br>\n",
    "1. [A Simple Model](#model)<br>\n",
    "1. [Fitting the Model](#fitting)<br>\n",
    "1. [Increasing Model Complexity](#complexity)<br>\n",
    "1. [Gradient Descent](#gd)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section A: Project<a id='project'></a>\n",
    "\n",
    "This week, each member of your team should conduct an EDA of a different dataset that you are considering for your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question A.1** In a few sentences, explain where and how you obtained this data and how it was collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question A.2** Briefly summarize the structure, granularity, scope, temporality and faithfulness (write 1-2 sentences for each of structure, granularity, etc). Is there any aspect of this dataset that is limiting, or any reason to question its validity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question A.3** Specify three data cleaning operations that you will have to perform on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Gradient Descent\n",
    "Before we dive into the data and the homework, let's set up our motivation for exploring gradient descent. So far, we've been finding model parameters for linear regression by defining a loss function: a function that we want to minimize. Specifically, this loss function has been the mean squared error (MSE) - the linear regression fitting that we did in homework 5 and lab 5 worked by solving for the $\\theta$ values that minimize the mean squared error of the training data. To minimize the MSE, we have to take its derivative, set it to zero, and solve for the parameters.<br>\n",
    "\n",
    "This process isn't always feasible. One reason for this is that when you have a problem with a lot of response variables (features), setting the MSE derivative to equal zero becomes computationally intensive and involves inverting a very large matrix; when you have a model with a more complex form than linear regression, finding a derivative of the loss function and setting it to zero can be difficult.  A second reason is that some of the loss functions you might encounter can't be massaged into a form that allows you to find the parameters algebraically.  <br>\n",
    "\n",
    "This is where gradient descent comes in! For complex models, or models with many features, it's a more efficient way to find the minimum of the loss function. \n",
    "\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c47e278e09d6b09f99edf34faa82fd31",
     "grade": false,
     "grade_id": "imports",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight') \n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 14\n",
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0de6ac714e983580ceea233f5986eccd",
     "grade": false,
     "grade_id": "utils-code",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use plot_3d helper function to help us visualize gradient\n",
    "from hw7_utils import plot_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Load Data\n",
    "For this homework, we'll be working with theoretical data.\n",
    "Load the data.csv file into a pandas dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load our sample data\n",
    "data = pd.read_csv('hw7_data.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1. A Simple Model<a id='model'></a>\n",
    "Let's start by examining our data and creating a simple model that can represent this data.<br>\n",
    "\n",
    "**Question 1.1** Define a function `scatter()` that produces a scatter plot. It should take as input the x and y values, and produce a scatter plot with axis labels. Then, plot the $x$ and $y$ data from the `data` df you loaded above.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(x, y):\n",
    "    \"\"\"\n",
    "    Generate a scatter plot using x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2:** Describe any significant observations about the distribution of the data. How can you describe the relationship between $x$ and $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "299421c38e8ce4106c697c6c0f73f479",
     "grade": false,
     "grade_id": "q1c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.3:** For now, let's assume that the data follows some linear model, parametrized by $\\theta$:\n",
    "\n",
    "$\\Large\n",
    "\\hat{y} = \\theta \\cdot x\n",
    "$\n",
    "\n",
    "Define a linear model function `linear_model()` that produces a value $\\hat{y}$ given $x$ and $\\theta$, where $x$ is a vector of observations and $\\theta$ is a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Returns the estimate of y given x and theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- the scalar theta\n",
    "    \"\"\"\n",
    "    y_hat = ... # YOUR CODE HERE\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e36884f54d707ac74716f6bb333fb8b",
     "grade": true,
     "grade_id": "q1c-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell, do not change it\n",
    "assert linear_model(0, 1) == 0\n",
    "assert linear_model(10, 10) == 100\n",
    "assert np.sum(linear_model(np.array([3, 5]), 3)) == 24\n",
    "assert linear_model(np.array([7, 8]), 4).mean() == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae7ab3ef4cc512c35ec71d3936de6519",
     "grade": false,
     "grade_id": "q1d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.4:** In class, we learned that the $L^2$ loss function (i.e., **the mean squared error**) is smooth and continuous. Let's use $L^2$ loss to find an optimal value for $\\theta$. First, define the $L^2$ loss function `l2_loss` below, that calculates the value of $L^2$ given a set of actual observations $y$ and predictions $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def l2_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the average l^2 loss given y and y_hat\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    return ... # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e52031bf14b7bbf16073d8c35dcd9a2",
     "grade": true,
     "grade_id": "q1d-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell, do not change it\n",
    "assert l2_loss(2, 1) == 1\n",
    "assert l2_loss(2, 0) == 4 \n",
    "assert l2_loss(5, 1) == 16\n",
    "assert l2_loss(np.array([5, 6]), np.array([1, 1])) == 20.5\n",
    "assert l2_loss(np.array([1, 1, 1]), np.array([4, 1, 4])) == 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96544f4c36a79984e5de4a4a487c31eb",
     "grade": false,
     "grade_id": "q1e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 1.5:** Write a function `l2_plot()` that produces a plot of $L^2$ loss as a function of the coefficient $\\theta$. Your function should take inputs $x$ and $y$, which are vectors of $x$ and $y$ observations, and input `thetas`, which is a list of possible thetas. You should end up with a plot of $\\theta$ values on the x-axis, and the $L^2$ loss corresponding with those $\\theta$ values on the y-axis.  Make sure to label your axes and add a title.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_plot(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Plots the average l2 loss for given x, y as a function of theta.\n",
    "    Use the functions you wrote for linear_model and l2_loss.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    thetas -- the vector containing different estimates of theta\n",
    "    \"\"\"\n",
    "    # Calculate the loss here for each value of theta\n",
    "    # Create your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6:** Run the function `l2_plot()` using the $x$ and $y$ values from dataframe `data` above and a list of `thetas` (you can define this range yourself - the `np.linspace()` function might be helpful here). \n",
    "\n",
    "What appears to be the optimal $\\theta$ value based on the visualization? We'll call this value $\\theta^*$.  Set the variable `theta_star_guess` to the value of $\\theta$ that appears to minimize our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = ... # define possible thetas\n",
    "l2_plot(...) # call your L^2 loss plotting function\n",
    "theta_star_guess = ... # Your guess here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed066111e2c8396152c7fa9154367ad5",
     "grade": true,
     "grade_id": "q1e-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert l2_loss(3, 2) == 1\n",
    "assert l2_loss(0, 10) == 100\n",
    "assert -3 <= theta_star_guess <= -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca181f0e94b22634effdb43518e0829",
     "grade": false,
     "grade_id": "q2a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 2: Fitting our Simple Model<a id='fitting'></a>\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data.\n",
    "\n",
    "**Question 2.1:** Let's confirm our visual findings for our optimal coefficient $\\theta^*$. First, identify the analytical solution for the optimal $\\theta^*$ that minimizes average $L^2$ loss. Of the three options, below, which correctly gives the formula that tells us what $\\theta^*$ is given $i$ observations of $x$ and $y$? Highlight your answer in <font color = \"red\">red</font> (double click this cell if you don't know how to do this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $$\\Large {\\theta}^* = \\frac{\\sum x_i + y_i}{\\sum x_i^2}$$ <br>\n",
    "2. $$\\Large {\\theta}^* = \\frac{\\sum x_iy_i}{\\sum x_i^2}$$ <br>\n",
    "3. $$\\Large {\\theta}^* = \\frac{\\sum x_iy_i}{\\sum x_i}$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c598f919a78d4641871731a7612b2883",
     "grade": false,
     "grade_id": "q2b",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.2:** \n",
    "Now that we have the analytic solution for $\\theta^*$, implement the function `find_theta` that calculates the numerical value of $\\theta^*$ based on our data $x$, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Find optimal theta given x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return ...\n",
    "\n",
    "t_star = find_theta(...) # Your code here to get theta star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27647b637e875e3bf095a08e5195ec36",
     "grade": true,
     "grade_id": "q2b-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell; do not change it\n",
    "print(f'theta_opt = {t_star}')\n",
    "assert -2.5 <= t_star <= -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d229dae97b016b9810da4ef0b1eb5b4a",
     "grade": false,
     "grade_id": "q2c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.3:** Now, let's plot our loss function again using the `l2_plot()` function. This time, add a vertical line at the optimal value of theta (i.e. plot the line $x = \\theta^*$). The function `plt.axvline()` is helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_plot(...) # plot loss function\n",
    "# add a vertical line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0ad935625ba2fdf262b65914e1d8bb0",
     "grade": false,
     "grade_id": "q2d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br> \n",
    "**Question 2.4:** We now have an optimal value for $\\theta$ that minimizes our loss. In the cell below, plot the scatter plot of the data from Question 1a (you can reuse the `scatter()` function here). Add the best-fit line $\\hat{y} = \\theta^* \\cdot x$ using the $\\theta^*$ you computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850b8b8fb3a916556a9748579d7535e3",
     "grade": false,
     "grade_id": "q2e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.5:** Great! It looks like our estimate for $\\theta$ is able to capture a lot of the data with a single parameter. Now let's try to plot the residual to see what we've missed.<br>  \n",
    "\n",
    "The residual is defined as $r=y-\\theta^* \\cdot x$. Below, write a function to find the residual and plot the residuals as a function of the independent variable in a scatter plot. Plot a horizontal line at $y=0$ to assist with visualization. Add axis labels.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    # calculate residual\n",
    "    r = ...\n",
    "    \n",
    "    # plot residual, including axis labels and vertical line at y=0\n",
    "\n",
    "visualize_residual(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdd940e38928a89f1e6bc9709a2dd8c0",
     "grade": false,
     "grade_id": "q2f",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 2.6:** What does the residual look like? Do you notice a relationship between $x$ and $r$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9d0bb728baaa970ecddb9d55cb73057",
     "grade": false,
     "grade_id": "part-3",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 3: Increasing Model Complexity<a id='complexity'></a>\n",
    "\n",
    "It looks like the remaining data is cosinusoidal, meaning our original data follows a linear function and a cosinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta_1x + cos(\\theta_2x)\n",
    "$$\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, or composed together, $\\vec{\\theta}$.\n",
    "\n",
    "Note that a generalized cosine function $a\\cos(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. We can assume that the scaling and shifting parameter ($a$ and $c$ in this case) are 1 and 0 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** In the following cell, **explain why we can assume the scaling parameter to be 1 and shifting parameter to be 0 based on the residual plot in Question 2e**. \n",
    "\n",
    "You might find the following code helpful in visualizing all three parameters.\n",
    "\n",
    "```python\n",
    "def plot_cos_generalized(a,b,c,label=None):\n",
    "    \"\"\"Plot a cosine function with three parameters\"\"\"\n",
    "    X = np.linspace(-5, 5)\n",
    "    Y = a * np.cos(b*X + c)\n",
    "    plt.plot(X, Y, ':',label=label)\n",
    "    plt.legend()\n",
    " ```\n",
    "\n",
    "You can try plotting: \n",
    "```python\n",
    "plot_cos_generalized(1,1,1, label='cos(x)')\n",
    "plot_cos_generalized(1,1,2, label='cos(x + 2)')\n",
    "plot_cos_generalized(1,2,2, label='cos(2x + 2)')\n",
    "plot_cos_generalized(2,2,2, label='2cos(2x + 2)')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2:** As in Question 1, write a function that predicts a value $\\hat{y}$ given an input $x$ based on our new model.\n",
    "\n",
    "*Hint:* Try to do this without using for loops. The `np.cos` function may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_model(x, theta_1, theta_2):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta_1 -- the scalar value theta_1\n",
    "    theta_2 -- the scalar value theta_2\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isclose(cos_model(1, 1, np.pi), 0))\n",
    "# Check that we accept x as arrays\n",
    "assert len(cos_model(x, 2, 2)) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3:** In this question your job is to match the left and right sides of the equations for:\n",
    "1. The $L^2$ loss for for the `cos` model, $\\hat{y} = \\theta_1x + cos(\\theta_2x)$.  We'll call that $L(x, y, \\theta_1, \\theta_2)$.\n",
    "2. The partial derivatives of the `cos` model loss functions, $\\frac{\\partial L }{\\partial \\theta_1}, \\frac{\\partial L }{\\partial \\theta_2}$. \n",
    "\n",
    "Notice that we now have $\\vec{x}$ and $\\vec{y}$ instead of $x$ and $y$. This means that when determining the loss function $L(x, y, \\theta_1, \\theta_2)$, you'll need to take the average of the squared losses for each $y_i$, $\\hat{y_i}$ pair.\n",
    "\n",
    "As your answer below, match the right side (letters) to the correct left sides (numbers).\n",
    "\n",
    "1. $L(x, y, \\theta_1, \\theta_2)$ <br>\n",
    "2. $\\frac{\\partial L}{\\partial \\theta_1}$ <br>\n",
    "3. $\\frac{\\partial L}{\\partial \\theta_2}$ <br>\n",
    "\n",
    "\n",
    "A. $\\frac{2}{n} \\sum_{i=1}^n (x_i y_i \\sin(\\theta_2 x_i) - \\theta_1 x_i ^ 2 \\sin(\\theta_2 x_i) - x_i \\cos(\\theta_2 x_i)\\sin(\\theta_2 x_i))$ <br>\n",
    "B. $-\\frac{2}{n} \\sum_{i=1}^n (x_i y_i - \\theta_1 x_i ^ 2 - x_i \\cos(\\theta_2 x_i))$<br>\n",
    "C.  $\\frac{1}{n} \\sum_{i=1}^n (y_i - \\theta_1 x_i - \\cos(\\theta_2 x_i)) ^ 2$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer*: <br>\n",
    "1. ... <br>\n",
    "2. ... <br>\n",
    "3. ... <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4:** Now, implement the functions `dt1` and `dt2`, which should compute $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ respectively. Use the formulas you wrote for $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ in the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $( \\theta_1, \\theta_2 )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt1(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt2(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def dt(x, y, theta):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    return np.array([\n",
    "        dt1(x, y, theta),\n",
    "        dt2(x, y, theta)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell, do not change it. Both outputs should be True\n",
    "print(np.isclose(dt1(x, y, [0, np.pi]), 153.0123127740174))\n",
    "print(np.isclose(dt2(x, y, [0, np.pi]), 0.8562500798403736))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e4eac82be751c35e57b2901bedbf82",
     "grade": false,
     "grade_id": "q4a",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Section 4: Gradient Descent<a id='gd'></a>\n",
    "Now try to solve for the optimal $\\theta^*$ analytically...\n",
    "\n",
    "**Just kidding!**\n",
    "\n",
    "You can try but we don't recommend it. When finding an analytic solution becomes difficult or impossible, we resort to alternative optimization methods for finding an approximate solution.\n",
    "\n",
    "So let's try implementing a numerical optimization method: gradient descent!\n",
    "\n",
    "\n",
    "**Question 4.1:** Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in array $x$, array $y$, and an initial value for $\\theta$ (`theta`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate that is the same at every time step. \n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the $L^2$ loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dd1f17360f63ded9d0f0dfe08581039",
     "grade": false,
     "grade_id": "init-t",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "def init_t():\n",
    "    \"\"\"Creates an initial theta [0, 1] as a starting point for gradient descent\"\"\"\n",
    "    return np.array([0,1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def grad_desc(x, y, theta, num_iter=20, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "030f63d30d7689a302db74c047221beb",
     "grade": true,
     "grade_id": "q4a-tests",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell, do not change them\n",
    "t = init_t() # set initial theta values to 0\n",
    "t_est, ts, loss = grad_desc(x, y, t)\n",
    "\n",
    "print(len(ts) == len(loss) == 20) # theta history and loss history are 20 items in them\n",
    "print(ts[0].shape == (2,)) # theta history contains theta values\n",
    "print(np.isscalar(loss[0])) # loss history is a list of scalar values, not vector\n",
    "\n",
    "print(loss[1] - loss[-1] > 0) # loss is decreasing\n",
    "\n",
    "print(np.allclose(np.sum(t_est), -0.75, atol=2e-1))  # theta_est should be close to our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7a4de4c820b43095f4a209102836032",
     "grade": false,
     "grade_id": "q4c",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.2:** Let's visually inspect our results of running gradient descent to optimize $\\theta$. Plot our x values with our model's predicted y values over the original scatter plot. Did gradient descent successfully optimize $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e435cdf01d7746aa75f8c02102c5cf19",
     "grade": false,
     "grade_id": "q4c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c4757865c7f7455890131927bd0ee5a",
     "grade": false,
     "grade_id": "q4c-answer-2",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# get predicted y values from cosinusoidal model based on thetas obtained through gradient descent\n",
    "# plot model's predicted values\n",
    "# plot actual observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Xeditable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12706d520b488ac0538bc4f722857ce",
     "grade": false,
     "grade_id": "q4d",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.3:** Let's visualize gradient descent to see how it converges. Plot the loss values on the y-axis and the iteration number on the x-axis for your gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d9f1c8753fb61ffb493a5cdea66712d",
     "grade": false,
     "grade_id": "q4e",
     "locked": false,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**Question 4.4:** Create a single plot that shows the iteration (x-axis) vs the loss value (y-axis) for different values of `alpha`: try using `alpha` = 0.01, `alpha` = 0.005, and `alpha` = 0.0005. Add a legend. How does the loss value change over different iterations when alpha varies? Based on what you know about gradient descent, why does the loss value change in this way?<br>\n",
    "\n",
    "*Note*: if you have a function that returns multiple values, but you only care about some of those values, you can use `_` to indicate that you don't want to save a given output. For instance, running: `_,_, loss = grad_desc(x, y, t)` would only save the return value for `loss_history`, and not `theta` or `theta_history`. This can save you some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7817a1e2e9b92ca01299629f7b27e3b0",
     "grade": true,
     "grade_id": "q4e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit\n",
    "How--and why--does your model change if you set the initial conditions in the `init_t` function to significantly different values (e.g., 0,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "----\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "+ Data 100 - HW 5: Modeling, Estimation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
