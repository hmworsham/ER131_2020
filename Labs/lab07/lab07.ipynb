{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list anyone you collaborated with on this workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7: Gradient Descent & Logistic Regression\n",
    "**This lab was distributed the week of October 12, 2020 and should be completed by Tuesday, 10/20/2020 at 11:59PM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the seventh lab of the semester!\n",
    "\n",
    "In this lab, we'll introduce the concept of gradient descent. Then, we'll move on to solving classification problems using logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore', FutureWarning) # Seaborn triggers warnings in scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure nice plotting defaults - (this must be done in a cell separate from %matplotlib call)\n",
    "plt.style.use('seaborn')\n",
    "sns.set_context('talk', font_scale=1)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll try fitting a simple linear regression. The data we're using comes from [California's urban water suppliers](https://data.ca.gov/dataset/drinking-water-public-water-system-operations-monthly-water-production-and-conservation-information). Run the cell below to load the data to dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/uw_supplier_data100319.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data provides monthly reports of water production from urban water suppliers in California. We'll see how well we can predict \"Production_Calculated\" (production for a given month from a given water supplier, standardized to units of gallons of water, from 2014-2019) based on \"2013_Production_Calculated\" (production for that same month in 2013 from the same water supplier, standardized to units of gallons of water). Run the cell below to see what this relationship looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"2013_Production_Calculated\"]\n",
    "y = df[\"Production_Calculated\"]\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel(\"2013 monthly production (gal)\")\n",
    "plt.ylabel(\"2014-2019 monthly production (gal)\")\n",
    "plt.title(\"California urban water supplier monthly production\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute Loss (L1 Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we went over in lecture, the **mean absolute error** takes in the absolute difference between each point and the prediction. \n",
    "\n",
    "It is defined as:\n",
    "$\\begin{aligned}L&= \\frac{1}{n} \\sum_{i = 1}^{n} |y_i − \\hat{y}| \\\\\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error is known as L1 loss, and we will use those two terms interchangeably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `avg_abs_loss()` which takes in a set of predicted y values `y_hat` and a set of observed y values `y` and calculates mean absolute error  (hint: there's a numpy function you'll probably find pretty handy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_absolute_loss(y_hat, y):\n",
    "    return ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell and check to make sure the computation makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([5, 7, 8, 9])\n",
    "y_hat_test = np.array([6, 9, 7, 6])\n",
    "assert avg_absolute_loss(y_hat_test, y_test) == 1.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the L1 loss to get a better sense of what's happening. We're going to assume that the relationship between monthly production for 2013-2019 ($y$) and the monthly production for 2013 ($x$) can be described using a linear model, with the form $\\hat{y} = \\theta_0 + \\theta_1*x$. We'll start with the assumption that $\\theta_0 = 0$, then plot the mean absolute error for different values of $\\theta_1$. In the cell below, we define a range for $\\theta_1$, then calculate the L1 loss for each $\\theta_1$ and plot the L1 loss vs. $\\theta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "theta_0 = 0\n",
    "theta_1 = np.arange(0,2,0.1)\n",
    "\n",
    "loss = np.full(len(theta_1),np.nan)\n",
    "\n",
    "\n",
    "for i in range(len(theta_1)):\n",
    "    y_hat = theta_0 + theta_1[i]*x\n",
    "    loss[i] = avg_absolute_loss(y_hat,y)\n",
    "\n",
    "# plot the loss\n",
    "plt.plot(theta_1, loss, label=\"L1 loss\")\n",
    "plt.xlabel(r\"Choice for $\\theta_1$\")\n",
    "plt.ylabel(r\"MAE\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot, what value of theta minimizes our loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_YOUR ANSWER HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we cannot minimize loss functions analytically, especially when our models get more complex. And as we have seen in the asynchronous modules, brute force minimization by just trying out a bunch of different theta values and seeing which one returns the least loss is also incredibly inefficient. \n",
    "\n",
    "Instead, we use a technique called gradient descent, which is explained in [Ch.11 of the DS100 Textbook](https://www.textbook.ds100.org/ch/11/gradient_descent_define.html) and in Homework 7. To remind you of the intuition: the slope of the tangent line tells us which direction to move $\\theta_1$ in order to decrease the loss. If the slope is negative, we want $\\theta_1$ to move in the positive direction. If the slope is positive, $\\theta_1$ should move in the negative direction. \n",
    "\n",
    "And mathematically, our formula is:\n",
    "\n",
    "$$\\theta^{(t+1)}_1 = \\theta^{(t)}_1 − \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} L(\\theta^{(t)}_1, \\textbf{y})$$\n",
    "\n",
    "Where $ \\theta^{(t)}_1$ is the current estimate, $ \\theta^{(t+1)}_1$ is the next estimate, and $\\alpha$ is the learning rate, or step size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the partial derivative (gradient) of L1 loss. Using this formula, write a function that takes in a theta value and the observed data points, and returns the gradient of L1 loss at that theta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\theta_1} L(\\theta_1, \\textbf{y})\n",
    "&= \\frac{1}{n} \\left( \\sum_{y_i < \\theta_1*x_i}x_i - \\sum_{y_i > \\theta_1*x_i} x_i \\right)\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE IN THE ELLIPSES\n",
    "def grad_abs_loss(theta_1, x, y):\n",
    "    x_greater = ... \n",
    "    x_less = ...\n",
    "    n = ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function called `minimize`, which iteratively calculates $\\theta$ until the values converge. In addition to $x$ and $y$, your function should take as input a loss function (e.g., the `avg_absolute_loss` you defined in Question 1.1).; a gradient loss function (e.g., the `grad_abs_loss` you defined in Question 3.1, and a value for $\\alpha$. You only need to fill in the parts to find `new_theta` using the mathematical formula for gradient descent:\n",
    "\n",
    "$$\\theta^{(t+1)}_1 = \\theta^{(t)}_1 − \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} L(\\theta_1^{(t)}, \\textbf{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def minimize(loss_fn, grad_loss_fn, x,y, alpha=1e-10, progress=True):\n",
    "    '''\n",
    "    Uses gradient descent to minimize loss_fn. Returns the minimizing value of\n",
    "    theta_hat.\n",
    "    '''\n",
    "    theta = 0\n",
    "    loss = np.array([])\n",
    "    while True:\n",
    "        if progress:\n",
    "            print(f'theta: {theta:.2f} | loss: {loss_fn(theta*x, y):.2f}')\n",
    "        loss = ... # Append new loss to loss array\n",
    "        gradient = ... # Calculate gradient\n",
    "        new_theta = ... # Find new theta estimate        \n",
    "        if len(loss) - len(np.unique(loss)) >= 10:\n",
    "            return new_theta\n",
    "        \n",
    "        print('Number of iterations: {}'.format(len(loss)))\n",
    "        \n",
    "        theta = new_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4** \n",
    "\n",
    "What is the purpose of the code in Lines 16-17 of the `minimize` function?\n",
    "\n",
    "`if len(loss) - len(np.unique(loss)) >= 10:\n",
    "            return new_theta`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see `minimize()` iteratively print each step in gradient descent and to find the minimizing theta for our small toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = minimize(avg_absolute_loss, grad_abs_loss, x,y)\n",
    "print(f'Minimizing theta: {theta}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5** \n",
    "\n",
    "What happens to $ \\hat {\\theta}$ and the solve time (as indicated by number of iterations) if you set $\\alpha$ equal to 1e-5? What about $\\alpha$ = 1e-11?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Section 2: Logistic regression\n",
    "\n",
    "In this lab, we'll do our first classification problem. With classification, we are working to predict response variables that are qualitative (i.e. categorical) rather than quantitative.\n",
    "\n",
    "Some real-life examples of classification, from ISLR, are:\n",
    "1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\n",
    "1. An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.\n",
    "1. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.\n",
    "\n",
    "Logistic regression is one method for solving classification problems.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Let's start with the simple example of a modelling problem where the categorical variable can take on two values. For example, take a look at example #2 above - a bank needs to determine whether a transaction is fraudulent, so it is trying to model a situation with two potential values of the response variable: either the transaction was fraudulent (we'll give this category a value of Y = 1) or it was not (we'll give this category a value of Y = 0).\n",
    "\n",
    "Since Y isn't a *continuous* variable - it can only take on a value of 0 or a value of 1, and these values don't represent any particular ordering - we use logistic regression, which models the *probability* that Y = 0 or Y = 1. We can write this probability as:\n",
    "\n",
    "$ Pr(Y = 1|X)$ \n",
    "\n",
    "The expression above, in words, means \"the probability that Y = 1 given X\". In our case, Y = 1 means the transaction was fradulent, and X is the matrix of predictor variables (transaction history, IP address, etc) that we're using to predict Y. This probability expression can be written using the following shorthand:\n",
    "\n",
    "$ Pr(Y = 1|X) = p(X)$ \n",
    "\n",
    "We *could* model this probability using a simple linear regression model, that would look something like this:\n",
    "\n",
    "$p(X) = \\beta_0 + \\beta_1X_1$\n",
    "\n",
    "As we'll see throughout the lab, however, simple linear regression doesn't really give us meaningful results when it comes to classification problems. Instead, logistic regression proposes a model that uses the logistic function, which looks something like this:\n",
    "\n",
    "$p(X) = \\frac{e^{\\beta_0 + \\beta_1X_1}}{1 + e^{\\beta_0 + \\beta_1X_1}}$\n",
    "\n",
    "The two model forms above (linear and logistic regression) show the model form when we have one feature $X_1$, but can be easily extended to multiple features. The key to the logistic function is that it can only evaluate to values between 0 and 1, so it will always represent a probability.\n",
    "\n",
    "Another important and related parameter is the **odds**. In statistics, the odds are defined as the probability of success over the probability of failure. If something is 50% likely to succeed, then its odds are 1 - there is an equal probability of success to failure. If it's 80% likely to succed, its odds are 0.8/(1-0.8) = 4 - there is a 4 to 1 chance of success to failure. The odds of $p(X)$ can be written as:\n",
    "\n",
    "$\\frac{p(X)}{1-p(X)}$\n",
    "\n",
    "The odds of $p(X)$ can take any value between 0 and infinity. The logistic function can be re-written in the following way:\n",
    "\n",
    "$log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1X_1$\n",
    "\n",
    "This should look super familiar! It looks *almost* like a linear regression. Basically, we can represent the log-odds (also called the logit) as a function that is linear in $X_1$. It's important to be careful with interpretation here, though - even though the relationship between the logit and $X$ is linear, the relationship between $p(X)$ and $X$ is not. A unit increase in $X_1$ in the formula above represents an increase equal to $\\beta_1$ in the *log-odds* of $p(X)$, which is the same as multiplying the odds by $e^{\\beta_1}$. The amount that $p(X)$ changes when $X$ changes depends on the current value of $X$. However, there is a simple interpretation to the signs of the coefficients - if $\\beta_1$ is positive, that means that an increase in $X_1$ leads to an increased probability of $p(X)$.\n",
    "\n",
    "The basic intuition for how to choose the $\\beta$ coefficients for logistic regression is similar to linear regression. With linear regression, we want to choose coefficients that make the prediction as close as possible to the observation $y$. With logistic regression, we want to choose coefficients that make the predicted probability (of, for example, a fraudulent transaction) as close as possible to the actual category of the transaction (1 for a fraudulent transaction, 0 for a non-fraudulent transaction). Logistic regression software functions work by maximizing something called the likelihood function, much like linear regression functions work by minimizing the mean squared error.\n",
    "\n",
    "**(Optional) Challenge:** In the explanation above, we asserted that:\n",
    "\n",
    "$log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1X_1$\n",
    "\n",
    "Using a little bit of algebra, can you prove the claim above - that the logistic function representation is equivalent to logit representation for the logistic regression model? I.e., start with the logistic function:\n",
    "\n",
    "$p(X) = \\beta_0 + \\beta_1X_1$\n",
    "\n",
    "and solve for\n",
    "\n",
    "$log\\left(\\frac{p(X)}{1-p(X)}\\right)$\n",
    "\n",
    "to get the logit representation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now that we've gone through the basics, we can load our data and try implementing logistic regression.\n",
    "\n",
    "In this section, we'll work with a modified version of the [ozone level detection dataset](https://archive.ics.uci.edu/ml/datasets/ozone+level+detection) from the UCI Machine Learning Repository, which uses temperature, wind speed, pressure, and other features to decide if a specific day was in fact a normal day or a high ground level ozone day. \n",
    "\n",
    "Run the cell below to load ozone.csv into dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this cell\n",
    "df = pd.read_csv('data/ozone.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the columns, we can infer that columns WSR0, WSR1, etc. are the hourly measurements for the wind speed, and the second-to-last column, `Class`, is the variable we want to predict. 0 is a  normal day and 1 is an ozone day. If you want more information on the features, you can read up on the description of the data [here](https://archive.ics.uci.edu/ml/datasets/ozone+level+detection). \n",
    "\n",
    "**Question 2.1** Create a scatter plot showing `Class` versus `WSR_PK`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** Run the cell below to fit a linear regression to the data shown above. Based on the plot and your understanding of the data, why would linear regression be a poor choice for a predictive model in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(df[[\"WSR_PK\"]], df[[\"Class\"]])\n",
    "\n",
    "plt.scatter(df.WSR_PK, df.Class)\n",
    "plt.plot(df.WSR_PK, lm.predict(df[[\"WSR_PK\"]]), c = \"y\")\n",
    "plt.xlabel(\"Wind speed\")\n",
    "plt.ylabel(\"Ozone day status\")\n",
    "plt.title(\"Linear regression fit to ozone status\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try building a predictive model using classification! We'll build a logistic regression model with statsmodels. We'll use the [`Logit()` function](https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html) function.\n",
    "\n",
    "As an aside: we could also use scikit-learn's `LogisticRegression()` function, but scikit-learn performs logistic regression using a penalty by default, while statsmodel uses maximum likelihood estimation to choose coefficients. Both are useful depending on how you want to solve for your coefficients!\n",
    "\n",
    "**Question 2.3** Run a logistic regression on the ozone dataset, using \"Class\" as the response variable and `WSR_PK` as the predictor. The code below gets you started. You may remember that statsmodels models require the `X` input to contain a column of ones to represent the intercept. Conveniently, the last column of our dataframe `df` is a column of ones called `intercept`. (If it didn't, we could use `np.ones()` to create a column of ones). Using the code below as a starting point, run a logistic regression that predicts `Class` given `WSR_PK` and `intercept`, and then print the parameters.\n",
    "\n",
    "If you can't remember the general process for how to fit models with statsmodels, look back at Question 12 in Lab 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "y = ...\n",
    "X = ...\n",
    "\n",
    "logit = sm.Logit(...)\n",
    "s = logit.fit()\n",
    "s.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these weights (i.e. parameters or coefficients) shown above to plot predicted probabilities. To do that, we have to be able to calculate probability from the weights. Recall the relationship:\n",
    "$$p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1+e^{\\beta_0 + \\beta_1X}}$$\n",
    "\n",
    "**Question 2.4** Write a function `prob_logit()` that returns a vector of probabilities p(X) given a vector of $\\beta$ values and a matrix of $X$ values. The function `np.exp()` will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_logit(params, X):\n",
    "    \"\"\"\n",
    "    This function calculates the probability p(X) given a set of weights (params) and a matrix of X values.\n",
    "    Arguments:\n",
    "    params, a vector of weights from a logistic regression fit\n",
    "    X, a matrix of predictor values with a number of columns equal to number of elements in params\n",
    "    \n",
    "    Returns:\n",
    "    p, a vector of probabilities with the same length as the number of rows in X\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE TO CALCULATE p\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions of output vector\n",
    "assert(len(prob_logit(s.params, X)) == X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5** The `assert` block above checked the dimensions of your output. In the cell below, check that all the values in your output vector `p` are between 0 and 1 (inclusive) - i.e. we want to make sure that we're calculated a probability! If you find that your values are not all greater than or equal to 0 and less than or equal to 1, double check the calculation in `prob_logit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6** Now that we have our probabilities, we can plot our predictor variable (`WSR_PK`) vs our response variable (`CLASS`) with a curve representing the probability of an ozone day at each value of `WSR_PK` overlayed. Create that plot below - you will end up with a scatter plot that looks like the one in 2.1, with a curve overlayed.\n",
    "\n",
    "Note that to avoid a plot where `matplotlib` just connects the dots and you have lines all over the place, you'll have to pass a *sorted* version of `WSR_PK` to your function `prob_logit()`. Below, we've defined a variable `X_plot` that sorts the `WSR_PK` values that you can use as an input (assuming that you wrote `prob_logit()` to take a matrix `X` as input that contains features and the intercept; if you wrote `prob_logit()` to take a matrix without an intercept, you'll have to modify `X_plot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = df[['WSR_PK', 'intercept']].sort_values(by = 'WSR_PK') # sorted X values\n",
    "\n",
    "# Your plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7** Visually, at what values of `WSR_PK` is our model pretty accurate? At what values of `WSR_PK` does it lose accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we used only one feature for our regression, logistic regression also works with multiple features and is a powerful tool for classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bibliography\n",
    "- DS100 - “Gradient Descent” - https://www.textbook.ds100.org/ch/11/gradient_descent_define.html \n",
    "- DS100 - “Absolute Loss” - https://www.textbook.ds100.org/ch/10/modeling_abs_huber.html\n",
    "- DS100 - “Models and Estimation” - http://www.ds100.org/fa18/assets/lectures/lec09/09-Models-and-Estimation-II.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Joshua Asuncion, Rebekah Tang. Edited by Jessica Katz.\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
